#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„CUDA LoRAå†…æ ¸ä¸Tritonç‰ˆæœ¬çš„ç­‰ä»·æ€§
"""

import os
import sys
import torch
import numpy as np

# æ·»åŠ vLLMè·¯å¾„
sys.path.insert(0, '/home/vllm')

from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


def test_cuda_triton_equivalence():
    """æµ‹è¯•CUDAå’ŒTriton LoRAå†…æ ¸çš„ç­‰ä»·æ€§"""
    
    print("ğŸ”§ æµ‹è¯•CUDAä¸Triton LoRAå†…æ ¸ç­‰ä»·æ€§")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„
    base_model_path = "hf_models/Qwen2.5-1.5B"
    lora1_model_path = "hf_models/Qwen2.5-1.5B-lora1"
    lora2_model_path = "hf_models/Qwen2.5-1.5B-lora2"
    
    # åˆå§‹åŒ–LLM
    os.environ["VLLM_USE_V1"] = "0"
    
    print("ğŸš€ åˆå§‹åŒ–vLLM...")
    llm = LLM(
        model=base_model_path,
        enable_lora=True,
        max_lora_rank=64,
        max_loras=2,
        max_model_len=256,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        enforce_eager=True,
        disable_custom_all_reduce=True,
        trust_remote_code=True,
        max_num_seqs=8,
    )
    
    # åˆ›å»ºLoRAè¯·æ±‚
    lora1_request = LoRARequest("lora1", 1, lora1_model_path)
    lora2_request = LoRARequest("lora2", 2, lora2_model_path)
    
    print(f"âœ… LoRAè¯·æ±‚åˆ›å»ºå®Œæˆ:")
    print(f"   LoRA 1: {lora1_request}")
    print(f"   LoRA 2: {lora2_request}")
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.0,
        max_tokens=10,
    )
    
    # æµ‹è¯•æ•°æ®
    test_prompts = [
        "Hello, how are you?",
        "What is the capital of France?",
        "Explain quantum computing.",
        "Write a short story about a robot.",
    ]
    
    test_lora_requests = [
        lora1_request,
        lora2_request,
        lora1_request,
        lora2_request,
    ]
    
    print(f"\nğŸ“Š æµ‹è¯•é…ç½®:")
    for i, (prompt, lora_req) in enumerate(zip(test_prompts, test_lora_requests)):
        print(f"   [{i+1}] {lora_req.lora_name}: {prompt[:30]}...")
    
    print(f"\nğŸ”¥ æ‰§è¡Œæ··åˆLoRAæ‰¹å¤„ç†...")
    print("âš¡ è¿™å°†è§¦å‘CUDA kernelså¹¶ä¸Tritonè¿›è¡Œæ¯”è¾ƒ!")
    
    try:
        # æ‰§è¡Œæ¨ç†
        outputs = llm.generate(
            test_prompts,
            sampling_params,
            lora_request=test_lora_requests
        )
        
        print(f"âœ… æ··åˆLoRAæ‰¹å¤„ç†æˆåŠŸ!")
        print(f"ğŸ“Š ç”Ÿæˆäº†{len(outputs)}ä¸ªè¾“å‡º")
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“‹ æ‰¹å¤„ç†ç»“æœ:")
        for i, (output, lora_req) in enumerate(zip(outputs, test_lora_requests)):
            generated_text = output.outputs[0].text.strip()
            print(f"[{lora_req.lora_name}] {output.prompt[:20]}... â†’ {generated_text[:30]}...")
            
        print(f"\nğŸ¯ ç­‰ä»·æ€§æµ‹è¯•å®Œæˆ!")
        print("âœ… å¦‚æœæ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼Œè¯´æ˜CUDAå†…æ ¸ä¸Tritonç‰ˆæœ¬ç­‰ä»·")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def main():
    print("ğŸ” CUDA vs Triton LoRAå†…æ ¸ç­‰ä»·æ€§æµ‹è¯•")
    print("=" * 60)
    
    success = test_cuda_triton_equivalence()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CUDAå†…æ ¸å·²æˆåŠŸä¿®å¤ä¸ºä¸Tritonç­‰ä»·")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
