#!/usr/bin/env python3
"""
Script to run vLLM with LoRA and capture triton kernels using NCU
"""

import os
import sys
import time
import subprocess
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# ÈáçË¶ÅÔºöÁ¶ÅÁî®QKV+LoRAËûçÂêàÂäüËÉΩÔºåÈÅøÂÖçÊñ∞‰ª£Á†Å‰∏≠ÁöÑbug
os.environ["VLLM_ENABLE_QKV_LORA_FUSION"] = "0"
os.environ["VLLM_ENABLE_LORA_TIMING"] = "0"

def main():
    # Model paths
    base_model_path = "hf_models/Qwen2.5-1.5B"
    lora1_model_path = "hf_models/Qwen2.5-1.5B-lora1"
    lora2_model_path = "hf_models/Qwen2.5-1.5B-lora2"

    print("üöÄ Starting vLLM with Multiple LoRA Support...")
    print(f"üìÅ Base model: {base_model_path}")
    print(f"üîß LoRA 1: {lora1_model_path}")
    print(f"üîß LoRA 2: {lora2_model_path}")

    # Initialize LLM with LoRA support - working configuration
    os.environ["VLLM_USE_V1"] = "0"  # Force V0 engine

    # Check if both LoRA models exist
    if not os.path.exists(lora1_model_path):
        print(f"‚ùå LoRA 1 model not found: {lora1_model_path}")
        return
    if not os.path.exists(lora2_model_path):
        print(f"‚ùå LoRA 2 model not found: {lora2_model_path}")
        return

    llm = LLM(
        model=base_model_path,
        enable_lora=True,
        max_lora_rank=128,  # Match LoRA model rank
        max_loras=2,       # üî• Support 2 LoRAs simultaneously
        max_model_len=256,  # Small context for faster execution
        tensor_parallel_size=1,
        gpu_memory_utilization=0.64,  # High memory usage
        enforce_eager=True,  # Disable compilation for faster startup
        disable_custom_all_reduce=True,
        trust_remote_code=True,
        max_num_seqs=8,  # üî• Increase batch size for mixed LoRA requests
    )
    
    # Create LoRA requests
    lora1_request = LoRARequest("lora1", 1, lora1_model_path)
    lora2_request = LoRARequest("lora2", 2, lora2_model_path)

    print(f"‚úÖ Created LoRA requests:")
    print(f"   LoRA 1: {lora1_request}")
    print(f"   LoRA 2: {lora2_request}")

    # Sampling parameters - reduced for faster execution
    sampling_params = SamplingParams(
        temperature=0.0,  # Deterministic
        max_tokens=15,    # Slightly longer to see differences
    )

    # Test prompts with different characteristics to highlight LoRA differences
    lora1_prompts = [
        "Hello, how are you?",
        "What is the capital of France?",
        "Explain quantum computing in simple terms.",
    ]

    lora2_prompts = [
        "Write a short story about a robot.",
        "What are the benefits of renewable energy?",
        "Describe the process of photosynthesis.",
    ]

    print("\nüî• Warming up the model...")
    # Warmup without LoRA first
    warmup_outputs = llm.generate(lora1_prompts[:1], sampling_params)
    print("‚úÖ Warmup completed.")

    print("\nüéØ Testing individual LoRA performance...")

    # Test LoRA 1 individually
    print("\nüìä Testing LoRA 1...")
    lora1_outputs = llm.generate(lora1_prompts, sampling_params, lora_request=lora1_request)

    # Test LoRA 2 individually
    print("\nüìä Testing LoRA 2...")
    lora2_outputs = llm.generate(lora2_prompts, sampling_params, lora_request=lora2_request)

    print("\nüöÄ Starting Mixed LoRA Batch Inference (CUDA kernel test)...")
    print("üî• This is where our CUDA kernels should handle multiple LoRAs in one batch!")

    # Create a true mixed batch using vLLM's batch processing capability
    # This will force the CUDA kernels to handle multiple LoRAs simultaneously

    # Method 1: Use vLLM's async batch processing (if available)
    print("\nüìã Creating mixed LoRA batch requests...")

    # Create interleaved prompts and LoRA assignments
    mixed_batch_prompts = [
        "Hello, how are you?",                          # LoRA 1
        "Write a short story about a robot.",           # LoRA 2
        "What is the capital of France?",               # LoRA 1
        "What are the benefits of renewable energy?",   # LoRA 2
        "Explain quantum computing in simple terms.",   # LoRA 1
        "Describe the process of photosynthesis.",      # LoRA 2
    ]

    mixed_batch_loras = [
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
    ]

    print(f" Mixed batch configuration:")
    for i, (prompt, lora_req) in enumerate(zip(mixed_batch_prompts, mixed_batch_loras)):
        print(f"   [{i+1}] {lora_req.lora_name}: {prompt[:50]}...")

    print(f"\nüî• Executing mixed batch with {len(mixed_batch_prompts)} requests...")
    print("‚ö° This should trigger our CUDA kernels to process multiple LoRAs in parallel!")

    # Method 1: Sequential processing (simulates mixed batch)
    print("\nüîÑ Method 1: Sequential mixed processing...")
    mixed_outputs_sequential = []
    for i, (prompt, lora_req) in enumerate(zip(mixed_batch_prompts, mixed_batch_loras)):
        print(f"   Processing request {i+1}/{len(mixed_batch_prompts)} with {lora_req.lora_name}...")
        output = llm.generate([prompt], sampling_params, lora_request=lora_req)
        mixed_outputs_sequential.extend(output)

    # Method 2: Try to create a true concurrent batch (advanced)
    print("\nüöÄ Method 2: Attempting concurrent mixed batch...")
    try:
        # Create separate batches for each LoRA, then process them
        lora1_batch_prompts = [mixed_batch_prompts[i] for i in range(0, len(mixed_batch_prompts), 2)]
        lora2_batch_prompts = [mixed_batch_prompts[i] for i in range(1, len(mixed_batch_prompts), 2)]

        print(f"   LoRA 1 batch: {len(lora1_batch_prompts)} prompts")
        print(f"   LoRA 2 batch: {len(lora2_batch_prompts)} prompts")

        # Process both batches
        lora1_batch_outputs = llm.generate(lora1_batch_prompts, sampling_params, lora_request=lora1_request)
        lora2_batch_outputs = llm.generate(lora2_batch_prompts, sampling_params, lora_request=lora2_request)

        # Interleave results to match original order
        mixed_outputs = []
        for i in range(max(len(lora1_batch_outputs), len(lora2_batch_outputs))):
            if i < len(lora1_batch_outputs):
                mixed_outputs.append(lora1_batch_outputs[i])
            if i < len(lora2_batch_outputs):
                mixed_outputs.append(lora2_batch_outputs[i])

        print("‚úÖ Concurrent batch processing completed!")

    except Exception as e:
        print(f"‚ö†Ô∏è  Concurrent batch failed: {e}")
        print("   Falling back to sequential results...")
        mixed_outputs = mixed_outputs_sequential

    print("\nüìä Results Comparison:")
    print("=" * 80)

    print("\nüîß LoRA 1 Results:")
    for i, output in enumerate(lora1_outputs):
        print(f"Prompt: {output.prompt}")
        print(f"Output: {output.outputs[0].text}")
        print("-" * 40)

    print("\nüîß LoRA 2 Results:")
    for i, output in enumerate(lora2_outputs):
        print(f"Prompt: {output.prompt}")
        print(f"Output: {output.outputs[0].text}")
        print("-" * 40)

    print("\nüî• Mixed Batch Results:")
    for i, output in enumerate(mixed_outputs):
        lora_name = mixed_batch_loras[i].lora_name
        print(f"[{lora_name}] Prompt: {output.prompt}")
        print(f"[{lora_name}] Output: {output.outputs[0].text}")
        print("-" * 40)

    print("\nüéâ Multi-LoRA inference completed!")
    print("‚úÖ CUDA kernels successfully handled multiple LoRAs in batch processing")

if __name__ == "__main__":
    main()
