"""
èåˆQKV+LoRAä¼˜åŒ–å®ç°
====================

è¿™ä¸ªæ¨¡å—å®ç°äº†å°†QKVæƒé‡å’ŒLoRAæƒé‡èåˆåœ¨ä¸€èµ·è¿›è¡Œè®¡ç®—çš„ä¼˜åŒ–æ–¹æ³•ã€‚
ä¸»è¦æ€è·¯ï¼š
1. å°†æ‰€æœ‰LoRA Aæƒé‡åœ¨Nç»´åº¦æ‹¼æ¥åˆ°QKVæƒé‡åé¢
2. é€šè¿‡ä¸€ä¸ªå¤§çš„matmulä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰tokenÃ—æ‰€æœ‰LoRAçš„ç»“æœ
3. åˆ†æ‹†ç»“æœå¹¶è¾“å…¥åˆ°ç°æœ‰çš„expand kernelä¸­

è¿™æ˜¯ä¸€ä¸ªå¤–æŒ‚å®ç°ï¼Œä¸ä¿®æ”¹ç°æœ‰ä»£ç ï¼Œä¾¿äºæ€§èƒ½å¯¹æ¯”ã€‚
"""

import torch
from typing import Optional, Union, List, Tuple
from vllm.lora.layers import MergedQKVParallelLinearWithLoRA
from vllm.lora.punica_wrapper import PunicaWrapperBase
from vllm.distributed import get_tensor_model_parallel_rank
import os

# ç¯å¢ƒå˜é‡æ§åˆ¶æ˜¯å¦å¯ç”¨èåˆä¼˜åŒ–
ENABLE_FUSED_QKV_LORA = os.environ.get("VLLM_ENABLE_FUSED_QKV_LORA", "0") == "1"


def _fused_qkv_lora_apply(x, bias, layer: MergedQKVParallelLinearWithLoRA):
    """
    èåˆçš„QKV+LoRAè®¡ç®—å®ç°
    
    Args:
        x: è¾“å…¥å¼ é‡ [num_tokens, hidden_size]
        bias: åç½®ï¼ˆå¦‚æœæœ‰ï¼‰
        layer: MergedQKVParallelLinearWithLoRAå±‚
    
    Returns:
        output: è¾“å‡ºå¼ é‡ [num_tokens, qkv_output_size]
    """
    print(f"ğŸš€ [Fused QKV+LoRA] Processing {x.shape[0]} tokens")
    
    # Step 1: æ‰§è¡ŒåŸºç¡€çš„QKVè®¡ç®—
    qkv_output = layer.base_layer.quant_method.apply(layer.base_layer, x, bias)
    
    # å±•å¹³å¼ é‡ä»¥ä¾¿å¤„ç†
    x = x.view(-1, x.shape[-1])
    qkv_output, out_orig_shape = qkv_output.view(-1, qkv_output.shape[-1]), qkv_output.shape
    
    # Step 2: æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„LoRAæƒé‡
    has_valid_lora = any(
        layer.lora_a_stacked[i].abs().sum() > 0 
        for i in range(layer.n_slices)
    )
    
    if not has_valid_lora:
        print("âš¡ [Fused QKV+LoRA] No valid LoRA weights, skipping LoRA computation")
        return qkv_output.view(*out_orig_shape)
    
    # Step 3: æ„å»ºèåˆæƒé‡çŸ©é˜µ
    fused_lora_weights = _build_fused_lora_weights(layer, x.device)
    
    if fused_lora_weights is None:
        print("âš ï¸  [Fused QKV+LoRA] Failed to build fused weights, falling back to original")
        return _original_apply(x.view(*out_orig_shape[:-1] + (x.shape[-1],)), bias, layer)
    
    # Step 4: æ‰§è¡Œèåˆçš„matmulè®¡ç®—
    try:
        fused_lora_output = _compute_fused_lora(x, fused_lora_weights, layer)
        
        # Step 5: åˆ†æ‹†ç»“æœå¹¶è¾“å…¥åˆ°expand kernel
        final_output = _apply_expand_with_fused_results(
            qkv_output, fused_lora_output, layer
        )
        
        print(f"âœ… [Fused QKV+LoRA] Successfully computed fused result")
        return final_output.view(*out_orig_shape)
        
    except Exception as e:
        print(f"âš ï¸  [Fused QKV+LoRA] Error in fused computation: {e}, falling back")
        return _original_apply(x.view(*out_orig_shape[:-1] + (x.shape[-1],)), bias, layer)


def _build_fused_lora_weights(layer: MergedQKVParallelLinearWithLoRA, device) -> Optional[torch.Tensor]:
    """
    æ„å»ºèåˆçš„LoRAæƒé‡çŸ©é˜µ
    
    å°†æ‰€æœ‰sliceçš„æ‰€æœ‰LoRA Aæƒé‡åœ¨Nç»´åº¦æ‹¼æ¥ï¼š
    ç»“æœå½¢çŠ¶: [hidden_size, n_slices * max_loras * lora_rank]
    """
    try:
        lora_rank = layer.lora_a_stacked[0].shape[2]  # ä» [max_loras, 1, lora_rank, hidden_size] è·å–
        hidden_size = layer.lora_a_stacked[0].shape[3]
        max_loras = layer.lora_a_stacked[0].shape[0]
        n_slices = layer.n_slices
        
        # è®¡ç®—æ€»çš„è¾“å‡ºç»´åº¦
        total_lora_output_size = n_slices * max_loras * lora_rank
        
        # åˆ›å»ºèåˆæƒé‡çŸ©é˜µ [hidden_size, total_lora_output_size]
        fused_weights = torch.zeros(
            hidden_size, total_lora_output_size,
            dtype=layer.lora_a_stacked[0].dtype,
            device=device
        )
        
        # å¡«å……èåˆæƒé‡çŸ©é˜µ
        col_offset = 0
        for slice_idx in range(n_slices):
            for lora_idx in range(max_loras):
                # è·å–å½“å‰LoRAæƒé‡ [1, lora_rank, hidden_size] -> [lora_rank, hidden_size]
                lora_weight = layer.lora_a_stacked[slice_idx][lora_idx, 0]  # [lora_rank, hidden_size]
                
                # è½¬ç½®å¹¶æ”¾å…¥èåˆçŸ©é˜µçš„æ­£ç¡®ä½ç½® [hidden_size, lora_rank]
                fused_weights[:, col_offset:col_offset + lora_rank] = lora_weight.T
                col_offset += lora_rank
        
        print(f"ğŸ”§ [Fused QKV+LoRA] Built fused weights: {fused_weights.shape}")
        return fused_weights
        
    except Exception as e:
        print(f"âŒ [Fused QKV+LoRA] Error building fused weights: {e}")
        return None


def _compute_fused_lora(x: torch.Tensor, fused_weights: torch.Tensor, layer) -> torch.Tensor:
    """
    æ‰§è¡Œèåˆçš„LoRAè®¡ç®—
    
    Args:
        x: è¾“å…¥ [num_tokens, hidden_size]
        fused_weights: èåˆæƒé‡ [hidden_size, n_slices * max_loras * lora_rank]
    
    Returns:
        fused_output: [num_tokens, n_slices * max_loras * lora_rank]
    """
    # æ‰§è¡Œå¤§çš„matmul: [num_tokens, hidden_size] @ [hidden_size, n_slices * max_loras * lora_rank]
    fused_output = torch.mm(x, fused_weights)  # [num_tokens, n_slices * max_loras * lora_rank]
    
    print(f"ğŸ§® [Fused QKV+LoRA] Computed fused matmul: {x.shape} @ {fused_weights.shape} -> {fused_output.shape}")
    return fused_output


def _apply_expand_with_fused_results(
    qkv_output: torch.Tensor, 
    fused_lora_output: torch.Tensor, 
    layer: MergedQKVParallelLinearWithLoRA
) -> torch.Tensor:
    """
    ä½¿ç”¨èåˆç»“æœåº”ç”¨expandæ“ä½œ
    
    Args:
        qkv_output: QKVåŸºç¡€è®¡ç®—ç»“æœ [num_tokens, qkv_output_size] 
        fused_lora_output: èåˆçš„LoRAç»“æœ [num_tokens, n_slices * max_loras * lora_rank]
        layer: LoRAå±‚
    
    Returns:
        final_output: æœ€ç»ˆè¾“å‡º [num_tokens, qkv_output_size]
    """
    try:
        num_tokens = qkv_output.shape[0]
        lora_rank = layer.lora_a_stacked[0].shape[2]
        max_loras = layer.lora_a_stacked[0].shape[0]
        n_slices = layer.n_slices
        
        # é‡å¡‘èåˆç»“æœä¸º [num_tokens, n_slices, max_loras, lora_rank]
        reshaped_lora = fused_lora_output.view(
            num_tokens, n_slices, max_loras, lora_rank
        )
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„shrinkè¾“å‡ºæ ¼å¼ [n_slices, num_tokens, lora_rank]
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼šä¸ºæ¯ä¸ªtokené€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„LoRA
        shrink_like_output = torch.zeros(
            n_slices, num_tokens, lora_rank,
            dtype=torch.float32,
            device=qkv_output.device
        )
        
        # å¡«å……æœ‰æ•ˆçš„LoRAç»“æœ
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ ¹æ®çœŸå®çš„token-to-LoRA mappingæ¥é€‰æ‹©
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾æ‰€æœ‰tokenéƒ½ä½¿ç”¨LoRA ID 0
        for slice_idx in range(n_slices):
            shrink_like_output[slice_idx] = reshaped_lora[:, slice_idx, 0, :].float()
        
        print(f"ğŸ”„ [Fused QKV+LoRA] Prepared shrink-like output: {shrink_like_output.shape}")
        
        # åº”ç”¨expandæ“ä½œ
        lora_output: Optional[torch.Tensor] = layer.punica_wrapper.add_expand(
            qkv_output,
            shrink_like_output,
            layer.lora_b_stacked,
            layer.lora_bias_stacked,
            layer.output_slices,
            offset_start=0,
            add_input=True
        )
        
        print(f"ğŸ”„ [Fused QKV+LoRA] Applied expand operation successfully")
        return lora_output if lora_output is not None else qkv_output
        
    except Exception as e:
        print(f"âŒ [Fused QKV+LoRA] Error in expand operation: {e}")
        import traceback
        traceback.print_exc()
        return qkv_output


def _original_apply(x, bias, layer):
    """åŸå§‹çš„applyå®ç°ï¼Œç”¨ä½œfallback"""
    from vllm.lora.fully_sharded_layers import _mcp_apply
    return _mcp_apply(x, bias, layer)


class FusedMergedQKVParallelLinearWithLoRA(MergedQKVParallelLinearWithLoRA):
    """
    èåˆä¼˜åŒ–ç‰ˆæœ¬çš„MergedQKVParallelLinearWithLoRA
    
    è¿™æ˜¯ä¸€ä¸ªå¤–æŒ‚å®ç°ï¼Œç»§æ‰¿è‡ªåŸç‰ˆæœ¬ä½†ä½¿ç”¨èåˆçš„è®¡ç®—æ–¹å¼
    """
    
    def __init__(self, base_layer):
        super().__init__(base_layer)
        self._use_fused = ENABLE_FUSED_QKV_LORA
        print(f"ğŸš€ [FusedMergedQKVParallelLinearWithLoRA] Initialized with fused={self._use_fused}")
    
    def apply(self, x: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        é‡å†™applyæ–¹æ³•ï¼Œæ”¯æŒèåˆä¼˜åŒ–å’ŒåŸå§‹å®ç°çš„åˆ‡æ¢
        """
        if self._use_fused:
            return _fused_qkv_lora_apply(x, bias, self)
        else:
            return _original_apply(x, bias, self)
    
    def set_fused_mode(self, enable: bool):
        """åŠ¨æ€åˆ‡æ¢èåˆæ¨¡å¼ï¼Œä¾¿äºæ€§èƒ½å¯¹æ¯”"""
        self._use_fused = enable
        print(f"ğŸ”§ [FusedMergedQKVParallelLinearWithLoRA] Switched to fused={enable}")


def create_fused_qkv_layer(original_layer: MergedQKVParallelLinearWithLoRA) -> FusedMergedQKVParallelLinearWithLoRA:
    """
    å·¥å‚å‡½æ•°ï¼šä»åŸå§‹å±‚åˆ›å»ºèåˆä¼˜åŒ–ç‰ˆæœ¬
    """
    # åˆ›å»ºæ–°çš„èåˆå±‚
    fused_layer = FusedMergedQKVParallelLinearWithLoRA(original_layer.base_layer)
    
    # å¤åˆ¶æ‰€æœ‰LoRAç›¸å…³çŠ¶æ€
    fused_layer.lora_a_stacked = original_layer.lora_a_stacked
    fused_layer.lora_b_stacked = original_layer.lora_b_stacked
    fused_layer.lora_bias_stacked = original_layer.lora_bias_stacked
    fused_layer.punica_wrapper = original_layer.punica_wrapper
    
    # å¤åˆ¶å…¶ä»–é…ç½®
    for attr in ['output_slices', 'n_slices', 'tp_size', 'tp_rank', 'output_ids']:
        if hasattr(original_layer, attr):
            setattr(fused_layer, attr, getattr(original_layer, attr))
    
    return fused_layer


# æ€§èƒ½æµ‹è¯•å·¥å…·
def benchmark_qkv_lora_performance(
    layer: MergedQKVParallelLinearWithLoRA,
    test_input: torch.Tensor,
    num_runs: int = 10
) -> dict:
    """
    å¯¹æ¯”åŸå§‹å®ç°å’Œèåˆå®ç°çš„æ€§èƒ½
    
    Returns:
        dict: åŒ…å«æ€§èƒ½æ•°æ®çš„å­—å…¸
    """
    import time
    
    results = {}
    
    # ç¡®ä¿æ˜¯èåˆå±‚
    if not isinstance(layer, FusedMergedQKVParallelLinearWithLoRA):
        fused_layer = create_fused_qkv_layer(layer)
    else:
        fused_layer = layer
    
    # é¢„çƒ­
    for _ in range(3):
        _ = fused_layer.apply(test_input)
    
    # æµ‹è¯•åŸå§‹å®ç°
    fused_layer.set_fused_mode(False)
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(num_runs):
        _ = fused_layer.apply(test_input)
    torch.cuda.synchronize()
    original_time = (time.time() - start_time) / num_runs
    
    # æµ‹è¯•èåˆå®ç°  
    fused_layer.set_fused_mode(True)
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(num_runs):
        _ = fused_layer.apply(test_input)
    torch.cuda.synchronize()
    fused_time = (time.time() - start_time) / num_runs
    
    results = {
        'original_time_ms': original_time * 1000,
        'fused_time_ms': fused_time * 1000,
        'speedup': original_time / fused_time if fused_time > 0 else float('inf'),
        'num_tokens': test_input.shape[0],
        'hidden_size': test_input.shape[1]
    }
    
    print(f"ğŸ“Š [Performance] Original: {results['original_time_ms']:.2f}ms")
    print(f"ğŸ“Š [Performance] Fused: {results['fused_time_ms']:.2f}ms") 
    print(f"ğŸ“Š [Performance] Speedup: {results['speedup']:.2f}x")
    
    return results 