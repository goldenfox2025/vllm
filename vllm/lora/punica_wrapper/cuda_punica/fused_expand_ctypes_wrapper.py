"""
Python ctypes wrapper for CUDA LoRA Fused Expand kernel
ä¸“é—¨å¤„ç†QKV+LoRAèåˆè®¡ç®—çš„expandæ“ä½œçš„Pythonæ¥å£
"""

import ctypes
import torch
import numpy as np
from typing import List, Tuple, Optional

# å°è¯•åŠ è½½CUDA LoRAåº“
try:
    from .ctypes_wrapper import cuda_lora_lib, C_LIB_AVAILABLE, _dtype_to_int
    FUSED_EXPAND_AVAILABLE = C_LIB_AVAILABLE
except ImportError:
    FUSED_EXPAND_AVAILABLE = False
    cuda_lora_lib = None

def cuda_lora_fused_expand_triton_interface(
    fused_shrink_input: torch.Tensor,     # [num_tokens, total_lora_rank]
    lora_b_weights: List[torch.Tensor],   # List of LoRA B weights for each slice
    output_tensor: torch.Tensor,         # [num_tokens, total_hidden_size]
    token_indices_sorted: torch.Tensor,   # Sorted token indices
    num_tokens_per_lora: torch.Tensor,    # Number of tokens per LoRA
    lora_token_start_loc: torch.Tensor,   # Start location for each LoRA
    lora_ids: torch.Tensor,               # Active LoRA IDs
    no_lora_flag: torch.Tensor,           # Flag indicating if LoRA is used
    slice_rank_info: List[dict],          # Each dict contains slice rank info
    offset_start: int = 0,
    add_inputs: bool = True
) -> bool:
    """
    CUDA LoRA Fused Expand kernelçš„Tritonå…¼å®¹æ¥å£
    
    Args:
        fused_shrink_input: èåˆshrinkç»“æœ [num_tokens, total_lora_rank]
                           æ ¼å¼ï¼štotal_lora_rank = max_loras * (slice0_rank + slice1_rank + slice2_rank)
        lora_b_weights: æ¯ä¸ªsliceçš„LoRA Bæƒé‡åˆ—è¡¨
        output_tensor: è¾“å‡ºå¼ é‡ [num_tokens, total_hidden_size]
        token_indices_sorted: æ’åºçš„tokenç´¢å¼•
        num_tokens_per_lora: æ¯ä¸ªLoRAçš„tokenæ•°é‡
        lora_token_start_loc: æ¯ä¸ªLoRAçš„tokenèµ·å§‹ä½ç½®
        lora_ids: æ´»è·ƒLoRA ID
        no_lora_flag: æ˜¯å¦ä½¿ç”¨LoRAçš„æ ‡å¿—
        slice_rank_info: æ¯ä¸ªsliceçš„rankä¿¡æ¯åˆ—è¡¨ï¼ˆä»…ç”¨äºè®¡ç®—å‚æ•°ï¼Œkernelå†…éƒ¨è®¡ç®—åç§»ï¼‰
        offset_start: è¾“å‡ºåç§»èµ·å§‹ä½ç½®
        add_inputs: æ˜¯å¦ç´¯åŠ åˆ°è¾“å‡º
        
    Returns:
        bool: æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    
    if not FUSED_EXPAND_AVAILABLE:
        print("âš ï¸  CUDA LoRA fused expand kernel not available")
        return False
    
    if no_lora_flag.item():
        # æ²¡æœ‰LoRAéœ€è¦å¤„ç†
        return True
    
    try:
        # åŸºæœ¬å‚æ•°
        num_tokens = fused_shrink_input.size(0)
        total_lora_rank = fused_shrink_input.size(1)
        total_hidden_size = output_tensor.size(1)
        num_slices = len(lora_b_weights)
        max_active_loras = lora_ids.size(0)
        
        print(f"ğŸ” [Fused Expand] å‚æ•°åˆ†æ:")
        print(f"   num_tokens: {num_tokens}")
        print(f"   total_lora_rank: {total_lora_rank}")
        print(f"   total_hidden_size: {total_hidden_size}")
        print(f"   num_slices: {num_slices}")
        print(f"   max_active_loras: {max_active_loras}")
        
        # æ„å»ºsliceä¿¡æ¯æ•°ç»„
        slice_starts = []
        slice_ranks = []
        hidden_sizes = []
        
        current_hidden_start = 0
        
        for i, info in enumerate(slice_rank_info):
            slice_idx = info['slice_idx']
            rank = info['rank']
            
            # è®¡ç®—sliceçš„hidden_size (ä»lora_b_weightsæ¨æ–­)
            if i < len(lora_b_weights):
                hidden_size = lora_b_weights[i].size(1)  # [lora_id, hidden_size, rank]
            else:
                hidden_size = 0
            
            slice_starts.append(current_hidden_start)
            slice_ranks.append(rank)
            hidden_sizes.append(hidden_size)
            
            current_hidden_start += hidden_size
            
            print(f"   Slice {slice_idx}: rank={rank}, hidden_size={hidden_size}, start={slice_starts[-1]}")
        
        # è½¬æ¢ä¸ºCUDAå¼ é‡
        slice_starts_tensor = torch.tensor(slice_starts, dtype=torch.int32, device=fused_shrink_input.device)
        slice_ranks_tensor = torch.tensor(slice_ranks, dtype=torch.int32, device=fused_shrink_input.device)
        hidden_sizes_tensor = torch.tensor(hidden_sizes, dtype=torch.int32, device=fused_shrink_input.device)
        
        # slice_rank_startså·²ç»ä¸éœ€è¦äº†ï¼Œä½†ä¸ºäº†ä¿æŒæ¥å£å…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªdummy tensor
        slice_rank_starts_tensor = torch.zeros(num_slices, dtype=torch.int32, device=fused_shrink_input.device)
        
        # æ„å»ºLoRA Bæƒé‡æŒ‡é’ˆæ•°ç»„
        lora_b_ptr_array = torch.zeros(num_slices, dtype=torch.int64, device=fused_shrink_input.device)
        lora_strides_d0 = torch.zeros(num_slices, dtype=torch.int32, device=fused_shrink_input.device)
        lora_strides_d1 = torch.zeros(num_slices, dtype=torch.int32, device=fused_shrink_input.device)
        lora_strides_d2 = torch.zeros(num_slices, dtype=torch.int32, device=fused_shrink_input.device)
        
        for i, lora_b in enumerate(lora_b_weights):
            lora_b_ptr_array[i] = lora_b.data_ptr()
            lora_strides_d0[i] = lora_b.stride(0)  # LoRA IDç»´åº¦
            lora_strides_d1[i] = lora_b.stride(1)  # hiddenç»´åº¦
            lora_strides_d2[i] = lora_b.stride(2)  # rankç»´åº¦
            
            print(f"   LoRA B[{i}] shape: {lora_b.shape}, strides: ({lora_b.stride(0)}, {lora_b.stride(1)}, {lora_b.stride(2)})")
        
        # æ•°æ®ç±»å‹è½¬æ¢
        input_dtype = _dtype_to_int(fused_shrink_input.dtype)
        output_dtype = _dtype_to_int(output_tensor.dtype)
        
        print(f"ğŸš€ [Fused Expand] è°ƒç”¨CUDA kernel...")
        
        # è°ƒç”¨CUDA kernel
        result = cuda_lora_lib.cuda_lora_fused_expand_c(
            # è¾“å…¥æ•°æ®æŒ‡é’ˆ
            ctypes.c_void_p(fused_shrink_input.data_ptr()),
            ctypes.c_void_p(lora_b_ptr_array.data_ptr()),
            ctypes.c_void_p(output_tensor.data_ptr()),
            
            # å…ƒæ•°æ®æŒ‡é’ˆ
            ctypes.c_void_p(token_indices_sorted.data_ptr()),
            ctypes.c_void_p(lora_ids.data_ptr()),
            ctypes.c_void_p(num_tokens_per_lora.data_ptr()),
            ctypes.c_void_p(lora_token_start_loc.data_ptr()),
            
            # sliceä¿¡æ¯
            ctypes.c_void_p(slice_starts_tensor.data_ptr()),
            ctypes.c_void_p(slice_ranks_tensor.data_ptr()),
            ctypes.c_void_p(slice_rank_starts_tensor.data_ptr()),  # dummyå‚æ•°
            
            # LoRAæƒé‡ä¿¡æ¯
            ctypes.c_void_p(lora_strides_d0.data_ptr()),
            ctypes.c_void_p(lora_strides_d1.data_ptr()),
            ctypes.c_void_p(lora_strides_d2.data_ptr()),
            ctypes.c_void_p(hidden_sizes_tensor.data_ptr()),
            
            # åŸºæœ¬å‚æ•°
            ctypes.c_int(max_active_loras),
            ctypes.c_int(num_tokens),
            ctypes.c_int(total_hidden_size),
            ctypes.c_int(num_slices),
            ctypes.c_int(1 if add_inputs else 0),
            
            # strideä¿¡æ¯
            ctypes.c_int(fused_shrink_input.stride(0)),
            ctypes.c_int(fused_shrink_input.stride(1)),
            ctypes.c_int(output_tensor.stride(0)),
            ctypes.c_int(output_tensor.stride(1)),
            
            # streamå’Œæ•°æ®ç±»å‹
            ctypes.c_void_p(0),  # ä½¿ç”¨é»˜è®¤æµ
            ctypes.c_int(input_dtype),
            ctypes.c_int(output_dtype)
        )
        
        if result != 0:
            print(f"âŒ CUDA fused expand kernel failed with code: {result}")
            return False
        
        print("ğŸš€ CUDA fused expand kernel completed successfully")
        return True
        
    except Exception as e:
        print(f"âŒ CUDA fused expand kernel error: {e}")
        return False


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print(f"CUDA LoRA Fused Expand availability: {FUSED_EXPAND_AVAILABLE}")
    if FUSED_EXPAND_AVAILABLE:
        print("âœ… CUDA LoRA fused expand kernel is available for testing")
    else:
        print("âŒ CUDA LoRA fused expand kernel is not available") 