import ctypes
import os
import torch
from typing import List, Optional

# Load the pure C library
C_LIB_AVAILABLE = False
cuda_c_lib = None

try:
    # Try to find the C library
    lib_path = os.path.join(os.path.dirname(__file__), "build", "libcuda_lora_c.so")
    if not os.path.exists(lib_path):
        # Try alternative location
        lib_path = os.path.join(os.path.dirname(__file__), "libcuda_lora_c.so")
    
    if os.path.exists(lib_path):
        cuda_c_lib = ctypes.CDLL(lib_path)
        
        # Define function signatures for ultimate fusion kernel
        cuda_c_lib.cuda_ultimate_fusion_c.argtypes = [
            ctypes.c_void_p,  # input_ptr
            ctypes.c_void_p,  # qkv_weights_ptr
            ctypes.c_void_p,  # lora_a_ptr_array
            ctypes.c_void_p,  # lora_b_ptr_array
            ctypes.c_void_p,  # output_ptr
            ctypes.c_void_p,  # token_indices_sorted_ptr
            ctypes.c_void_p,  # lora_ids_ptr
            ctypes.c_void_p,  # num_tokens_per_lora_ptr
            ctypes.c_void_p,  # lora_token_start_loc_ptr
            ctypes.c_void_p,  # slice_starts_ptr
            ctypes.c_void_p,  # lora_ranks_ptr
            ctypes.c_int,     # max_active_loras
            ctypes.c_int,     # num_tokens
            ctypes.c_int,     # hidden_size
            ctypes.c_int,     # qkv_output_size
            ctypes.c_int,     # num_slices
            ctypes.c_int,     # max_rank
            ctypes.c_int,     # input_stride0
            ctypes.c_int,     # input_stride1
            ctypes.c_int,     # qkv_stride0
            ctypes.c_int,     # qkv_stride1
            ctypes.c_int,     # lora_a_stride0
            ctypes.c_int,     # lora_a_stride1
            ctypes.c_int,     # lora_a_stride2
            ctypes.c_int,     # lora_b_stride0
            ctypes.c_int,     # lora_b_stride1
            ctypes.c_int,     # lora_b_stride2
            ctypes.c_int,     # output_stride0
            ctypes.c_int,     # output_stride1
            ctypes.c_void_p,  # stream_ptr
            ctypes.c_int,     # input_dtype
            ctypes.c_int,     # output_dtype
        ]
        cuda_c_lib.cuda_ultimate_fusion_c.restype = ctypes.c_int
        
        C_LIB_AVAILABLE = True
        print(f"Ultimate fusion CUDA library loaded: {lib_path}")
    else:
        print(f"C library not found at: {lib_path}")

except Exception as e:
    print(f"Failed to load C library for ultimate fusion: {e}")

# å…³é”®ä¿®å¤7: æŒ‰ç…§vLLMæ¨¡å¼å®ç°é¢„åˆ†é…+ç¼“å­˜ç­–ç•¥
# å‚è€ƒvLLMçš„_LORA_A_PTR_DICTå’Œ_LORA_B_PTR_DICTè®¾è®¡

# å…¨å±€æŒ‡é’ˆç¼“å­˜å­—å…¸ - æ¨¡ä»¿vLLMçš„è®¾è®¡
_LORA_A_PTR_DICT = {}
_LORA_B_PTR_DICT = {}
_SLICE_STARTS_DICT = {}
_LORA_RANKS_DICT = {}

# é¢„åˆ†é…çš„å›ºå®šç¼“å†²åŒº - ç¡®ä¿CUDA Graphå…¼å®¹æ€§
_DEVICE_BUFFERS = {}

def _get_device_buffers(device, max_loras=8, max_slices=8):
    """è·å–è®¾å¤‡çº§åˆ«çš„é¢„åˆ†é…ç¼“å†²åŒº - æ¨¡ä»¿vLLMçš„é¢„åˆ†é…ç­–ç•¥"""
    device_id = device.index if hasattr(device, 'index') else 0
    
    if device_id not in _DEVICE_BUFFERS:
        # é¢„åˆ†é…æ‰€æœ‰å¿…è¦çš„tensor - ä¸€æ¬¡æ€§åˆ›å»ºï¼Œæ°¸ä¸é‡Šæ”¾
        buffers = {
            'lora_a_ptr_buffer': torch.zeros(max_slices, max_loras, dtype=torch.int64, device=device),
            'lora_b_ptr_buffer': torch.zeros(max_slices, max_loras, dtype=torch.int64, device=device),
            'slice_starts_buffer': torch.zeros(max_slices, dtype=torch.int32, device=device),
            'lora_ranks_buffer': torch.zeros(max_loras, dtype=torch.int32, device=device),
            'max_loras': max_loras,
            'max_slices': max_slices,
        }
        _DEVICE_BUFFERS[device_id] = buffers
        print(f"ğŸ”§ åˆ›å»ºè®¾å¤‡{device_id}é¢„åˆ†é…ç¼“å†²åŒº: max_loras={max_loras}, max_slices={max_slices}")
    
    return _DEVICE_BUFFERS[device_id]

def _get_lora_ptr_cached(lora_stacked, device, is_a_weights=True):
    """è·å–LoRAæŒ‡é’ˆ - ä½¿ç”¨ç¼“å­˜ç­–ç•¥ï¼Œæ¨¡ä»¿vLLMçš„_get_lora_a_ptrè®¾è®¡"""
    cache_dict = _LORA_A_PTR_DICT if is_a_weights else _LORA_B_PTR_DICT
    weight_type = "A" if is_a_weights else "B"
    
    # åˆ›å»ºç¼“å­˜é”® - åŸºäºtensoråœ°å€
    cache_key = tuple(tensor.data_ptr() for tensor in lora_stacked)
    
    if cache_key not in cache_dict:
        print(f"ğŸ”§ é¦–æ¬¡åˆ›å»ºLoRA {weight_type}æŒ‡é’ˆç¼“å­˜: key={len(cache_key)}ä¸ªslice")
        
        # è·å–é¢„åˆ†é…ç¼“å†²åŒº
        buffers = _get_device_buffers(device, max_loras=8, max_slices=len(lora_stacked))
        
        if is_a_weights:
            ptr_buffer = buffers['lora_a_ptr_buffer']
        else:
            ptr_buffer = buffers['lora_b_ptr_buffer']
        
        # è®¡ç®—å¹¶å­˜å‚¨æŒ‡é’ˆ
        for slice_id, lora_tensor in enumerate(lora_stacked):
            max_loras = lora_tensor.shape[0]
            for lora_id in range(max_loras):
                # è®¡ç®—æ¯ä¸ªLoRAçš„æŒ‡é’ˆåç§»
                base_ptr = lora_tensor.data_ptr()
                offset = lora_id * lora_tensor.stride(0)
                element_size = lora_tensor.element_size()
                final_ptr = base_ptr + offset * element_size
                
                # å­˜å‚¨åˆ°é¢„åˆ†é…ç¼“å†²åŒº
                if slice_id < buffers['max_slices'] and lora_id < buffers['max_loras']:
                    ptr_buffer[slice_id, lora_id] = final_ptr
        
        # ç¼“å­˜ç»“æœ - å­˜å‚¨ç¼“å†²åŒºå¼•ç”¨
        cache_dict[cache_key] = ptr_buffer
        print(f"âœ… LoRA {weight_type}æŒ‡é’ˆç¼“å­˜åˆ›å»ºå®Œæˆ")
    
    return cache_dict[cache_key]

def _get_slice_starts_cached(output_slices, device):
    """è·å–slice starts - ä½¿ç”¨ç¼“å­˜ç­–ç•¥"""
    cache_key = tuple(output_slices)
    
    if cache_key not in _SLICE_STARTS_DICT:
        print(f"ğŸ”§ é¦–æ¬¡åˆ›å»ºslice_startsç¼“å­˜: {output_slices}")
        
        # è·å–é¢„åˆ†é…ç¼“å†²åŒº
        buffers = _get_device_buffers(device, max_slices=len(output_slices))
        slice_starts_buffer = buffers['slice_starts_buffer']
        
        # è®¡ç®—å¹¶å­˜å‚¨slice starts
        cumulative_size = 0
        for i, size in enumerate(output_slices):
            if i < buffers['max_slices']:
                slice_starts_buffer[i] = cumulative_size
            cumulative_size += size
        
        # ç¼“å­˜ç»“æœ - å­˜å‚¨slice startsçš„æ•°å€¼åˆ—è¡¨ï¼ˆç”¨äºè®¡ç®—ï¼‰å’Œtensorï¼ˆç”¨äºå†…æ ¸ï¼‰
        _SLICE_STARTS_DICT[cache_key] = {
            'tensor': slice_starts_buffer,
            'values': [cumulative_size := cumulative_size - size + (cumulative_size := cumulative_size - cumulative_size + sum(output_slices[:i+1])) - sum(output_slices[:i]) for i, size in enumerate(output_slices)],
            'total_size': sum(output_slices)
        }
        # é‡æ–°è®¡ç®—values
        values = []
        cumulative = 0
        for size in output_slices:
            values.append(cumulative)
            cumulative += size
        _SLICE_STARTS_DICT[cache_key]['values'] = values
        
        print(f"âœ… slice_startsç¼“å­˜åˆ›å»ºå®Œæˆ")
    
    return _SLICE_STARTS_DICT[cache_key]

def _get_lora_ranks_cached(lora_stacked, lora_ids, device):
    """è·å–LoRA ranks - ä½¿ç”¨ç¼“å­˜ç­–ç•¥"""
    # è®¡ç®—ranks
    max_rank = max(lora_a.shape[2] for lora_a in lora_stacked) if lora_stacked else 16
    cache_key = (max_rank, len(lora_ids))
    
    if cache_key not in _LORA_RANKS_DICT:
        print(f"ğŸ”§ é¦–æ¬¡åˆ›å»ºlora_ranksç¼“å­˜: max_rank={max_rank}, num_loras={len(lora_ids)}")
        
        # è·å–é¢„åˆ†é…ç¼“å†²åŒº
        buffers = _get_device_buffers(device, max_loras=max(8, len(lora_ids)))
        ranks_buffer = buffers['lora_ranks_buffer']
        
        # è®¾ç½®ranks - ç®€åŒ–ä¸ºæ‰€æœ‰LoRAä½¿ç”¨ç›¸åŒrank
        for i in range(min(len(lora_ids), buffers['max_loras'])):
            ranks_buffer[i] = max_rank
        
        # ç¼“å­˜ç»“æœ - å­˜å‚¨tensorå’Œæ•°å€¼
        _LORA_RANKS_DICT[cache_key] = {
            'tensor': ranks_buffer,
            'max_rank': max_rank
        }
        print(f"âœ… lora_ranksç¼“å­˜åˆ›å»ºå®Œæˆ")
    
    return _LORA_RANKS_DICT[cache_key]

def cuda_ultimate_fusion_interface(
    inputs: torch.Tensor,                    # [num_tokens, hidden_size]
    qkv_weights: torch.Tensor,               # [qkv_output_size, hidden_size]
    lora_a_stacked: tuple[torch.Tensor, ...], # tuple of LoRA A weights for each slice
    lora_b_stacked: tuple[torch.Tensor, ...], # tuple of LoRA B weights for each slice
    output_slices: tuple[int, ...],          # output size for each slice
    token_indices_sorted: torch.Tensor,
    num_tokens_per_lora: torch.Tensor,
    lora_token_start_loc: torch.Tensor,
    lora_ids: torch.Tensor,
    lora_ranks: Optional[torch.Tensor] = None,
    stream: Optional[int] = None,
) -> torch.Tensor:
    """
    ç»ˆæèåˆå†…æ ¸çš„Pythonæ¥å£ - çœŸæ­£çš„é›¶åŠ¨æ€åˆ†é…ç‰ˆæœ¬
    
    å…³é”®è®¾è®¡ï¼š
    1. ä½¿ç”¨å…¨å±€ç¼“å­˜å­—å…¸å­˜å‚¨æŒ‡é’ˆä¿¡æ¯ï¼ˆæ¨¡ä»¿_LORA_A_PTR_DICTï¼‰
    2. é¢„åˆ†é…æ‰€æœ‰tensorï¼Œé¿å…CUDA Graph captureæœŸé—´çš„åŠ¨æ€åˆ†é…
    3. åœ¨captureæœŸé—´ç»å¯¹ä¸åˆ›å»ºä»»ä½•tensorï¼Œä¸è°ƒç”¨tensor.item()ï¼Œä¸åšsliceæ“ä½œ
    """
    device = inputs.device
    
    # CUDA graph captureçŠ¶æ€æ£€æµ‹
    is_capturing = torch.cuda.is_current_stream_capturing()
    
    if not is_capturing:
        print(f"ğŸ”§ ç»ˆæèåˆè°ƒç”¨ - é›¶åŠ¨æ€åˆ†é…ç­–ç•¥")
        print(f"ğŸ”§ CUDA Graph Capturing: {is_capturing}")
    
    # éªŒè¯è¾“å…¥tensorçš„è®¾å¤‡å’Œè¿ç»­æ€§ - ä½†ä¸åšä»»ä½•ä¿®æ”¹
    for name, tensor in [
        ("inputs", inputs),
        ("qkv_weights", qkv_weights),
        ("token_indices_sorted", token_indices_sorted),
        ("num_tokens_per_lora", num_tokens_per_lora),
        ("lora_token_start_loc", lora_token_start_loc),
        ("lora_ids", lora_ids),
    ]:
        if not tensor.is_cuda:
            raise ValueError(f"{name} must be on CUDA device, got {tensor.device}")
        if not tensor.is_contiguous():
            raise ValueError(f"{name} must be contiguous")
    
    # éªŒè¯LoRAæƒé‡ - ä½†ä¸åšä»»ä½•ä¿®æ”¹
    lora_a_processed = []
    lora_b_processed = []
    
    for i, (lora_a, lora_b) in enumerate(zip(lora_a_stacked, lora_b_stacked)):
        if not lora_a.is_cuda or not lora_b.is_cuda:
            raise ValueError(f"LoRA weights must be on CUDA device")
        if not lora_a.is_contiguous() or not lora_b.is_contiguous():
            raise ValueError(f"LoRA weights must be contiguous")
        
        lora_a_processed.append(lora_a)
        lora_b_processed.append(lora_b)
        
        if not is_capturing:
            print(f"ğŸ”§ slice[{i}]: lora_a.shape={lora_a.shape}, lora_b.shape={lora_b.shape}")
    
    # åŸºæœ¬å‚æ•°
    num_tokens = inputs.shape[0]
    hidden_size = inputs.shape[1]
    qkv_output_size = qkv_weights.shape[0]
    num_slices = len(lora_a_processed)
    
    # åˆ›å»ºè¾“å‡ºtensor
    output = torch.zeros(num_tokens, qkv_output_size, dtype=inputs.dtype, device=device)
    
    # å…³é”®ï¼šä½¿ç”¨ç¼“å­˜ç­–ç•¥è·å–æ‰€æœ‰é¢„åˆ†é…çš„tensor
    if not is_capturing:
        print(f"ğŸ”§ è·å–ç¼“å­˜çš„æŒ‡é’ˆå’Œå…ƒæ•°æ®...")
    
    # è·å–LoRAæŒ‡é’ˆç¼“å­˜
    lora_a_ptr_buffer = _get_lora_ptr_cached(lora_a_processed, device, is_a_weights=True)
    lora_b_ptr_buffer = _get_lora_ptr_cached(lora_b_processed, device, is_a_weights=False)
    
    # è·å–slice startsç¼“å­˜
    slice_starts_info = _get_slice_starts_cached(output_slices, device)
    slice_starts = slice_starts_info['tensor']
    slice_values = slice_starts_info['values']  # é¢„è®¡ç®—çš„æ•°å€¼ï¼Œé¿å…tensor.item()
    
    # è·å–lora ranksç¼“å­˜
    if lora_ranks is None:
        lora_ranks_info = _get_lora_ranks_cached(lora_a_processed, lora_ids, device)
        lora_ranks = lora_ranks_info['tensor']
        max_rank = lora_ranks_info['max_rank']  # é¢„è®¡ç®—çš„æ•°å€¼
    else:
        if not lora_ranks.is_cuda or lora_ranks.device != device:
            raise ValueError("lora_ranks must be on correct CUDA device")
        if not lora_ranks.is_contiguous():
            raise ValueError("lora_ranks must be contiguous")
        max_rank = 16  # é»˜è®¤å€¼ï¼Œé¿å…è®¿é—®tensor
    
    # æ£€æŸ¥åº“å¯ç”¨æ€§
    if not C_LIB_AVAILABLE:
        raise RuntimeError("Ultimate fusion C library not available")
    
    # Strideè®¡ç®—
    input_stride0 = inputs.stride(0)
    input_stride1 = inputs.stride(1)
    qkv_stride0 = qkv_weights.stride(0)
    qkv_stride1 = qkv_weights.stride(1)
    output_stride0 = output.stride(0)
    output_stride1 = output.stride(1)
    
    # LoRA strides - ä½¿ç”¨åŸå§‹4D tensorçš„stride
    if len(lora_a_processed) > 0:
        lora_a = lora_a_processed[0]
        lora_a_stride0 = lora_a.stride(0)  # lora_idç»´åº¦
        lora_a_stride1 = lora_a.stride(1)  # 1ç»´åº¦ï¼ˆé€šå¸¸ä¸º1ï¼‰
        lora_a_stride2 = lora_a.stride(2)  # rankç»´åº¦
        lora_a_stride3 = lora_a.stride(3)  # hiddenç»´åº¦
        
        lora_b = lora_b_processed[0]
        lora_b_stride0 = lora_b.stride(0)  # lora_idç»´åº¦
        lora_b_stride1 = lora_b.stride(1)  # 1ç»´åº¦ï¼ˆé€šå¸¸ä¸º1ï¼‰
        lora_b_stride2 = lora_b.stride(2)  # outputç»´åº¦
        lora_b_stride3 = lora_b.stride(3)  # rankç»´åº¦
        
        # è°ƒæ•´strideä»¥åŒ¹é…å†…æ ¸æœŸæœ›çš„3Då¸ƒå±€
        final_lora_a_stride0 = lora_a_stride0
        final_lora_a_stride1 = lora_a_stride2  # rankç»´åº¦
        final_lora_a_stride2 = lora_a_stride3  # hiddenç»´åº¦
        
        final_lora_b_stride0 = lora_b_stride0
        final_lora_b_stride1 = lora_b_stride2  # outputç»´åº¦
        final_lora_b_stride2 = lora_b_stride3  # rankç»´åº¦
    else:
        # é»˜è®¤å€¼
        final_lora_a_stride0 = final_lora_a_stride1 = final_lora_a_stride2 = 1
        final_lora_b_stride0 = final_lora_b_stride1 = final_lora_b_stride2 = 1
    
    # æ•°æ®ç±»å‹æ˜ å°„
    dtype_map = {torch.float16: 0, torch.bfloat16: 1, torch.float32: 2}
    input_dtype = dtype_map.get(inputs.dtype, 0)
    output_dtype = dtype_map.get(output.dtype, 0)
    
    # è·å–CUDAæµ
    if stream is None:
        stream = torch.cuda.current_stream().cuda_stream
    
    # å…³é”®ï¼šç°åœ¨æ‰€æœ‰tensoréƒ½æ˜¯é¢„åˆ†é…çš„ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„æ•°å€¼é¿å…tensorè®¿é—®
    if not is_capturing:
        print(f"ğŸ”§ è°ƒç”¨ç»ˆæèåˆå†…æ ¸ - ä½¿ç”¨é¢„åˆ†é…tensorå’Œé¢„è®¡ç®—æ•°å€¼...")
    
    # ä¸ºæ¯ä¸ªsliceè°ƒç”¨å†…æ ¸
    for slice_id in range(num_slices):
        if not is_capturing:
            print(f"ğŸ”§ å¤„ç†slice {slice_id}/{num_slices}")
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨é¢„è®¡ç®—çš„æ•°å€¼ï¼Œé¿å…tensor.item()è°ƒç”¨
        slice_start = slice_values[slice_id]
        if slice_id + 1 < len(slice_values):
            slice_end = slice_values[slice_id + 1]
        else:
            slice_end = slice_values[-1] + output_slices[-1]  # æ€»å¤§å°
        slice_size = slice_end - slice_start
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨é¢„åˆ†é…çš„tensor viewsï¼Œé¿å…åŠ¨æ€sliceåˆ›å»º
        # æˆ‘ä»¬éœ€è¦åˆ›å»ºå›ºå®šå¤§å°çš„è§†å›¾ï¼Œä¸èƒ½ä½¿ç”¨åŠ¨æ€slicing
        
        # è·å–å½“å‰sliceçš„æŒ‡é’ˆæ•°ç»„ - ä½¿ç”¨é¢„åˆ†é…çš„ç¼“å†²åŒº
        slice_lora_a_ptrs = lora_a_ptr_buffer[slice_id]  # [max_loras]
        slice_lora_b_ptrs = lora_b_ptr_buffer[slice_id]  # [max_loras]
        
        if not is_capturing:
            print(f"   sliceèŒƒå›´: [{slice_start}:{slice_end}], size={slice_size}")
        
        # è®¡ç®—QKVæƒé‡å’Œè¾“å‡ºçš„æŒ‡é’ˆåç§»
        qkv_element_size = qkv_weights.element_size()
        output_element_size = output.element_size()
        
        qkv_slice_ptr = qkv_weights.data_ptr() + slice_start * qkv_weights.stride(0) * qkv_element_size
        output_slice_ptr = output.data_ptr() + slice_start * output.stride(1) * output_element_size
        
        # è°ƒç”¨Cåº“å‡½æ•° - ä½¿ç”¨é¢„åˆ†é…çš„tensorå’ŒæŒ‡é’ˆè®¡ç®—
        try:
            result = cuda_c_lib.cuda_ultimate_fusion_c(
                ctypes.c_void_p(inputs.data_ptr()),
                ctypes.c_void_p(qkv_slice_ptr),  # è®¡ç®—çš„sliceæŒ‡é’ˆ
                ctypes.c_void_p(slice_lora_a_ptrs.data_ptr()),  # é¢„åˆ†é…çš„æŒ‡é’ˆæ•°ç»„
                ctypes.c_void_p(slice_lora_b_ptrs.data_ptr()),  # é¢„åˆ†é…çš„æŒ‡é’ˆæ•°ç»„
                ctypes.c_void_p(output_slice_ptr),  # è®¡ç®—çš„sliceè¾“å‡ºæŒ‡é’ˆ
                ctypes.c_void_p(token_indices_sorted.data_ptr()),
                ctypes.c_void_p(lora_ids.data_ptr()),
                ctypes.c_void_p(num_tokens_per_lora.data_ptr()),
                ctypes.c_void_p(lora_token_start_loc.data_ptr()),
                ctypes.c_void_p(slice_starts.data_ptr()),
                ctypes.c_void_p(lora_ranks.data_ptr()),
                ctypes.c_int(len(lora_ids)),  # max_active_loras
                ctypes.c_int(num_tokens),
                ctypes.c_int(hidden_size),
                ctypes.c_int(slice_size),     # å½“å‰sliceçš„è¾“å‡ºå¤§å°
                ctypes.c_int(1),              # num_slices = 1 (å•ç‹¬å¤„ç†æ¯ä¸ªslice)
                ctypes.c_int(max_rank),       # ä½¿ç”¨é¢„è®¡ç®—çš„max_rank
                ctypes.c_int(input_stride0),
                ctypes.c_int(input_stride1),
                ctypes.c_int(qkv_weights.stride(0)),
                ctypes.c_int(qkv_weights.stride(1)),
                ctypes.c_int(final_lora_a_stride0),
                ctypes.c_int(final_lora_a_stride1),
                ctypes.c_int(final_lora_a_stride2),
                ctypes.c_int(final_lora_b_stride0),
                ctypes.c_int(final_lora_b_stride1),
                ctypes.c_int(final_lora_b_stride2),
                ctypes.c_int(output_stride0),
                ctypes.c_int(output_stride1),
                ctypes.c_void_p(stream),
                ctypes.c_int(input_dtype),
                ctypes.c_int(output_dtype),
            )
            
            if result != 0:
                raise RuntimeError(f"Ultimate fusion kernel failed for slice {slice_id} with code {result}")
                
        except Exception as e:
            if not is_capturing:
                print(f"âŒ Slice {slice_id} å¤±è´¥: {e}")
            raise
    
    # åŒæ­¥ç¡®ä¿æ‰€æœ‰è®¡ç®—å®Œæˆ - ä½†åªåœ¨écaptureæœŸé—´
    if not is_capturing:
        torch.cuda.synchronize()
        print(f"âœ… ç»ˆæèåˆå®Œæˆ: output.shape={output.shape}")
    
    return output

# ä¿ç•™æ—§çš„å‡½æ•°ç”¨äºå‘åå…¼å®¹
def _get_or_create_pointer_buffers(device, max_slices=8):
    """å‘åå…¼å®¹å‡½æ•°"""
    return _get_device_buffers(device, max_loras=8, max_slices=max_slices)

def test_ultimate_fusion():
    """æµ‹è¯•ç»ˆæèåˆå†…æ ¸"""
    if not C_LIB_AVAILABLE:
        print("âŒ Ultimate fusion library not available")
        return False
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available")
        return False
    
    print("ğŸ§ª Testing ultimate fusion kernel...")
    
    # æµ‹è¯•é…ç½®
    num_tokens = 4
    hidden_size = 8
    qkv_output_size = 12  # Q(4) + K(4) + V(4)
    rank = 2
    num_slices = 3  # Q, K, V
    
    device = torch.device('cuda:0')
    dtype = torch.float16
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    inputs = torch.randn(num_tokens, hidden_size, dtype=dtype, device=device)
    qkv_weights = torch.randn(qkv_output_size, hidden_size, dtype=dtype, device=device)
    
    # åˆ›å»ºLoRAæƒé‡ (æ¯ä¸ªsliceä¸€ä¸ª)
    lora_a_stacked = tuple(
        torch.randn(1, 1, rank, hidden_size, dtype=dtype, device=device)  # [max_loras, 1, rank, hidden]
        for _ in range(num_slices)
    )
    lora_b_stacked = tuple(
        torch.randn(1, 1, 4, rank, dtype=dtype, device=device)  # [max_loras, 1, slice_output, rank]
        for _ in range(num_slices)
    )
    
    output_slices = (4, 4, 4)  # Q, K, Vå„4ç»´
    
    # åˆ›å»ºç®€å•çš„Punicaå…ƒæ•°æ®ï¼ˆæ‰€æœ‰tokenä½¿ç”¨åŒä¸€ä¸ªLoRAï¼‰
    token_indices_sorted = torch.arange(num_tokens, dtype=torch.int32, device=device)
    num_tokens_per_lora = torch.tensor([num_tokens], dtype=torch.int32, device=device)
    lora_token_start_loc = torch.tensor([0], dtype=torch.int32, device=device)
    lora_ids = torch.tensor([0], dtype=torch.int32, device=device)
    
    try:
        # è°ƒç”¨ç»ˆæèåˆå†…æ ¸
        output = cuda_ultimate_fusion_interface(
            inputs, qkv_weights, lora_a_stacked, lora_b_stacked, output_slices,
            token_indices_sorted, num_tokens_per_lora, lora_token_start_loc, lora_ids
        )
        
        print(f"âœ… Ultimate fusion kernel test passed!")
        print(f"ğŸ“Š Output shape: {output.shape}")
        print(f"ğŸ“ˆ Output stats: min={output.min():.3f}, max={output.max():.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Ultimate fusion kernel test failed: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Testing ultimate fusion ctypes wrapper...")
    
    if test_ultimate_fusion():
        print("ğŸ‰ Ultimate fusion wrapper works!")
        print("\nğŸ”§ This is the ULTIMATE optimization - everything in one kernel!")
    else:
        print("âŒ Ultimate fusion wrapper failed!") 