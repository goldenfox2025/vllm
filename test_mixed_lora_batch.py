#!/usr/bin/env python3
"""
æµ‹è¯•vLLMçš„æ··åˆLoRAæ‰¹å¤„ç†åŠŸèƒ½ï¼Œè§‚å¯Ÿ2048 tokensçš„æ¥æº
"""

import os
import sys
import torch

# æ·»åŠ vLLMè·¯å¾„
sys.path.insert(0, '/home/vllm')

from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


def test_mixed_lora_batch():
    """æµ‹è¯•æ··åˆLoRAæ‰¹å¤„ç†"""
    
    print("ğŸ§ª æµ‹è¯•vLLMæ··åˆLoRAæ‰¹å¤„ç†")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„
    base_model_path = "hf_models/Qwen2.5-1.5B"
    lora1_model_path = "hf_models/Qwen2.5-1.5B-lora1"
    lora2_model_path = "hf_models/Qwen2.5-1.5B-lora2"
    
    # åˆå§‹åŒ–LLM
    os.environ["VLLM_USE_V1"] = "0"
    
    print("ğŸš€ åˆå§‹åŒ–vLLM...")
    llm = LLM(
        model=base_model_path,
        enable_lora=True,
        max_lora_rank=64,
        max_loras=2,       # æ”¯æŒ2ä¸ªLoRA
        max_model_len=256,  # é€‚ä¸­çš„context
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        enforce_eager=True,
        disable_custom_all_reduce=True,
        trust_remote_code=True,
        max_num_seqs=8,    # å¢å¤§batch size
    )
    
    # åˆ›å»ºLoRAè¯·æ±‚
    lora1_request = LoRARequest("lora1", 1, lora1_model_path)
    lora2_request = LoRARequest("lora2", 2, lora2_model_path)
    
    print(f"âœ… LoRAè¯·æ±‚åˆ›å»ºå®Œæˆ:")
    print(f"   LoRA 1: {lora1_request}")
    print(f"   LoRA 2: {lora2_request}")
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.0,
        max_tokens=20,
    )
    
    # æµ‹è¯•1: å•ä¸ªLoRAæ‰¹å¤„ç†
    print(f"\\nğŸ“Š æµ‹è¯•1: å•ä¸ªLoRAæ‰¹å¤„ç†")
    single_lora_prompts = [
        "Hello, how are you?",
        "What is the capital of France?",
        "Explain quantum computing.",
    ]
    
    print("ğŸ”„ å¤„ç†å•ä¸ªLoRAæ‰¹å¤„ç†...")
    single_outputs = llm.generate(single_lora_prompts, sampling_params, lora_request=lora1_request)
    print(f"âœ… å•ä¸ªLoRAæ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº†{len(single_outputs)}ä¸ªè¾“å‡º")
    
    # æµ‹è¯•2: æ··åˆLoRAæ‰¹å¤„ç†ï¼ˆå…³é”®æµ‹è¯•ï¼‰
    print(f"ğŸ“Š æµ‹è¯•2: æ··åˆLoRAæ‰¹å¤„ç† - è¿™é‡Œåº”è¯¥çœ‹åˆ°2048 tokens!")
    mixed_prompts = [
        "Hello, how are you?",                          # LoRA 1
        "Write a short story about a robot.",           # LoRA 2
        "What is the capital of France?",               # LoRA 1
        "What are the benefits of renewable energy?",   # LoRA 2
        "Explain quantum computing in simple terms.",   # LoRA 1
        "Describe the process of photosynthesis.",      # LoRA 2
        "How does machine learning work?",              # LoRA 1
        "What is the theory of relativity?",           # LoRA 2
        "What is the theory of relativity?",  
        "What is the theory of relativity?",  
        "What is the theory of relativity?",  
        "What is the theory of relativity?",  
    ]
    
    mixed_lora_requests = [
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora2_request,
        lora2_request,
        lora2_request,
        lora2_request,
    ]
    
    print(f"ğŸ”¥ æ··åˆæ‰¹å¤„ç†é…ç½®:")
    for i, (prompt, lora_req) in enumerate(zip(mixed_prompts, mixed_lora_requests)):
        print(f"   [{i+1}] {lora_req.lora_name}: {prompt[:40]}...")
    
    print(f"ğŸš€ æ‰§è¡Œæ··åˆLoRAæ‰¹å¤„ç†...")
    print("âš¡ è¿™é‡Œåº”è¯¥è§¦å‘æˆ‘ä»¬çš„CUDA kernelså¹¶æ˜¾ç¤ºå®é™…çš„tokenæ•°é‡!")
    
    try:
        # å…³é”®æµ‹è¯•ï¼šä½¿ç”¨list of LoRA requests
        mixed_outputs = llm.generate(
            mixed_prompts,
            sampling_params,
            lora_request=mixed_lora_requests  # ğŸ”¥ æ··åˆLoRAè¯·æ±‚åˆ—è¡¨
        )
        
        print(f"âœ… æ··åˆLoRAæ‰¹å¤„ç†æˆåŠŸ!")
        print(f"ğŸ“Š ç”Ÿæˆäº†{len(mixed_outputs)}ä¸ªè¾“å‡º")
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ“‹ æ··åˆæ‰¹å¤„ç†ç»“æœ:")
        for i, (output, lora_req) in enumerate(zip(mixed_outputs, mixed_lora_requests)):
            print(f"[{lora_req.lora_name}] {output.prompt[:30]}... â†’ {output.outputs[0].text}")
            
    except Exception as e:
        print(f"âŒ æ··åˆLoRAæ‰¹å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å›é€€åˆ°é¡ºåºå¤„ç†
        print("\\nğŸ”„ å›é€€åˆ°é¡ºåºå¤„ç†...")
        mixed_outputs = []
        for prompt, lora_req in zip(mixed_prompts, mixed_lora_requests):
            output = llm.generate([prompt], sampling_params, lora_request=lora_req)
            mixed_outputs.extend(output)
    
    print(f"ğŸ¯ æµ‹è¯•å®Œæˆ!")

def main():
    print("ğŸ” vLLMæ··åˆLoRAæ‰¹å¤„ç†è°ƒè¯•å·¥å…·")
    print("=" * 60)
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    
    # è¿è¡Œæµ‹è¯•
    test_mixed_lora_batch()

if __name__ == "__main__":
    main()
