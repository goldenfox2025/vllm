#!/usr/bin/env python3
"""
QKV+LoRAèåˆæ€§èƒ½æ—¶é—´æµ‹è¯•è„šæœ¬ï¼ˆå¤šLoRAå¹¶å‘ç‰ˆæœ¬ï¼‰
æ”¯æŒ2-6ä¸ªLoRAçš„çœŸæ­£å¹¶å‘åœºæ™¯æµ‹è¯•ï¼Œæ¨¡ä»¿benchmark_serving.pyçš„æ‰¹å¤„ç†æ–¹å¼

ä½¿ç”¨æ–¹æ³•ï¼š
export VLLM_ENABLE_QKV_LORA_FUSION=1
export VLLM_ENABLE_LORA_TIMING=1
python test_qkv_lora_timing.py --num-loras 4 --num-requests 20 --max-tokens 50
"""

import argparse
import os
import sys
import time
import torch
import random
import shutil
import glob
from pathlib import Path
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

def setup_performance_environment():
    """è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒå˜é‡"""
    print("ğŸ”§ è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")
    
    # æ ¸å¿ƒæ€§èƒ½ç¯å¢ƒå˜é‡
    performance_env = {
        "VLLM_ENABLE_QKV_LORA_FUSION": "1",  # å¯ç”¨QKV+LoRAèåˆ
        "VLLM_ENABLE_LORA_TIMING": "1",      # å¯ç”¨è¯¦ç»†æ—¶é—´æµ‹é‡
        "VLLM_USE_V1": "0",                  # ä½¿ç”¨V0å¼•æ“
    }
    
    for key, value in performance_env.items():
        os.environ[key] = value
        print(f"   âœ… {key} = {value}")
    
    return performance_env

def find_existing_loras(model_dir: str, num_loras: int) -> list[str]:
    """ä»æ¨¡å‹ç›®å½•ä¸­æŸ¥æ‰¾ç°æœ‰çš„LoRAï¼Œä¸åˆ›å»ºå‰¯æœ¬"""
    print(f"ğŸ” åœ¨ {model_dir} ä¸­æŸ¥æ‰¾ç°æœ‰çš„ {num_loras} ä¸ªLoRA...")
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„LoRAç›®å½•
    possible_patterns = [
        "*lora*",
        "*LoRA*", 
        "*LORA*"
    ]
    
    found_loras = []
    for pattern in possible_patterns:
        lora_paths = glob.glob(os.path.join(model_dir, pattern))
        for path in lora_paths:
            if os.path.isdir(path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„LoRAç›®å½•ï¼ˆåŒ…å«adapter_config.jsonï¼‰
                if os.path.exists(os.path.join(path, "adapter_config.json")):
                    found_loras.append(path)
    
    # å»é‡å¹¶æ’åº
    found_loras = sorted(list(set(found_loras)))
    
    print(f"ğŸ” æ‰¾åˆ° {len(found_loras)} ä¸ªLoRA:")
    for i, lora_path in enumerate(found_loras):
        print(f"   {i+1}. {os.path.basename(lora_path)}: {lora_path}")
    
    if len(found_loras) < num_loras:
        print(f"âš ï¸ åªæ‰¾åˆ° {len(found_loras)} ä¸ªLoRAï¼Œä½†éœ€è¦ {num_loras} ä¸ª")
        print(f"ğŸ’¡ å°†é‡å¤ä½¿ç”¨ç°æœ‰LoRAä»¥è¾¾åˆ°æ‰€éœ€æ•°é‡")
        
        # é‡å¤ä½¿ç”¨ç°æœ‰LoRAç›´åˆ°è¾¾åˆ°æ‰€éœ€æ•°é‡
        while len(found_loras) < num_loras:
            found_loras.extend(found_loras[:min(len(found_loras), num_loras - len(found_loras))])
    
    # åªè¿”å›æ‰€éœ€æ•°é‡çš„LoRA
    selected_loras = found_loras[:num_loras]
    
    print(f"âœ… æœ€ç»ˆé€‰æ‹©çš„ {len(selected_loras)} ä¸ªLoRA:")
    for i, lora_path in enumerate(selected_loras):
        print(f"   {i+1}. {os.path.basename(lora_path)}")
    
    return selected_loras

def create_performance_test_llm(model_path: str, max_loras: int):
    """åˆ›å»ºä¼˜åŒ–çš„LLMå®ä¾‹ç”¨äºæ€§èƒ½æµ‹è¯•"""
    print(f"ğŸš€ åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•LLM (æ”¯æŒ{max_loras}ä¸ªLoRA)...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    
    llm = LLM(
        model=model_path,
        enable_lora=True,
        max_lora_rank=128,           # æ”¯æŒè¾ƒå¤§çš„LoRA rank
        max_loras=max_loras,         # åŠ¨æ€æ”¯æŒçš„LoRAæ•°é‡
        max_model_len=256,           # æ›´å°çš„ä¸Šä¸‹æ–‡ä»¥åŠ å¿«æµ‹è¯•
        tensor_parallel_size=1,
        gpu_memory_utilization=0.65, # ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
        enforce_eager=True,          # ç¦ç”¨ç¼–è¯‘ï¼Œä¾¿äºæµ‹é‡çœŸå®æ€§èƒ½
        disable_custom_all_reduce=True,
        trust_remote_code=True,
        max_num_seqs=16,             # é€‚ä¸­çš„æ‰¹å¤„ç†å¤§å°
    )
    
    print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
    return llm

def generate_test_prompts(num_requests: int) -> list[str]:
    """ç”Ÿæˆæµ‹è¯•prompts"""
    base_prompts = [
        "Hello, how are you today?",
        "What is the capital of France?", 
        "Explain machine learning briefly.",
        "Write a short story about a robot.",
        "What are the benefits of renewable energy?",
        "Describe the process of photosynthesis.",
        "How do computers work?",
        "What is quantum computing?",
        "Explain artificial intelligence.",
        "How does the internet work?",
    ]
    
    # å¾ªç¯ä½¿ç”¨promptsç›´åˆ°è¾¾åˆ°æ‰€éœ€æ•°é‡
    prompts = []
    for i in range(num_requests):
        prompt = base_prompts[i % len(base_prompts)]
        # æ·»åŠ ä¸€äº›å˜åŒ–ä½¿æ¯ä¸ªè¯·æ±‚ç•¥æœ‰ä¸åŒ
        if i >= len(base_prompts):
            prompt = f"Request {i+1}: {prompt}"
        prompts.append(prompt)
    
    return prompts

def measure_inference_time(llm, prompts: list[str], lora_request, sampling_params, method_name: str) -> dict:
    """ç²¾ç¡®æµ‹é‡æ¨ç†æ—¶é—´çš„è¾…åŠ©å‡½æ•°"""
    print(f"â±ï¸ æµ‹é‡ {method_name} æ¨ç†æ—¶é—´...")
    
    # é¢„çƒ­ - é‡è¦ï¼ç¡®ä¿GPU kernelså·²ç»åˆå§‹åŒ–
    print("ğŸ”¥ é¢„çƒ­GPU kernels...")
    warmup_outputs = llm.generate([prompts[0]], sampling_params, lora_request=lora_request)
    torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
    
    # æ¸…ç†
    torch.cuda.empty_cache()
    time.sleep(0.1)
    
    # æ­£å¼æµ‹é‡
    start_time = time.perf_counter()
    torch.cuda.synchronize()  # å¼€å§‹å‰åŒæ­¥
    
    outputs = llm.generate(prompts, sampling_params, lora_request=lora_request)
    
    torch.cuda.synchronize()  # å®ŒæˆååŒæ­¥
    end_time = time.perf_counter()
    
    inference_time = end_time - start_time
    total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)
    throughput = total_tokens / inference_time if inference_time > 0 else 0
    
    # æ”¶é›†ç”Ÿæˆçš„æ–‡æœ¬
    generated_texts = []
    for i, output in enumerate(outputs):
        prompt = prompts[i] if i < len(prompts) else "N/A"
        generated_text = output.outputs[0].text
        generated_texts.append({
            'prompt': prompt,
            'generated': generated_text,
            'tokens': len(output.outputs[0].token_ids)
        })
    
    print(f"   {method_name} æ—¶é—´: {inference_time:.4f}s")
    print(f"   {method_name} æ€»tokens: {total_tokens}")
    print(f"   {method_name} ååé‡: {throughput:.1f} tokens/s")
    
    return {
        'time': inference_time,
        'tokens': total_tokens,
        'throughput': throughput,
        'method': method_name,
        'generated_texts': generated_texts
    }

def run_concurrent_lora_benchmark(
    llm, 
    lora_requests: list[LoRARequest], 
    num_requests: int,
    max_tokens: int,
    test_name: str
) -> dict:
    """è¿è¡ŒçœŸæ­£çš„å¹¶å‘å¤šLoRAåŸºå‡†æµ‹è¯•ï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼‰"""
    print(f"\nğŸ”¥ å¼€å§‹å¹¶å‘å¤šLoRAåŸºå‡†æµ‹è¯•: {test_name}")
    print(f"ğŸš€ LoRAæ•°é‡: {len(lora_requests)}, è¯·æ±‚æ•°é‡: {num_requests}")
    print("=" * 60)
    
    # ç”Ÿæˆæµ‹è¯•prompts
    test_prompts = generate_test_prompts(num_requests)
    
    # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ†é…LoRAï¼ˆè½®è¯¢æ–¹å¼ç¡®ä¿å‡åŒ€åˆ†å¸ƒï¼‰
    assigned_loras = []
    assigned_prompts = []
    
    for i, prompt in enumerate(test_prompts):
        lora_idx = i % len(lora_requests)
        assigned_loras.append(lora_requests[lora_idx])
        assigned_prompts.append(prompt)
        print(f"   [{i+1:2d}] {lora_requests[lora_idx].lora_name}: {prompt[:40]}...")
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.0,  # ä½¿ç”¨è´ªå©ªè§£ç å‡å°‘éšæœºæ€§ï¼Œä¾¿äºæ€§èƒ½å¯¹æ¯”
        max_tokens=max_tokens,
    )
    
    print(f"\nğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ€»è¯·æ±‚æ•°: {len(assigned_prompts)}")
    print(f"   LoRAåˆ†å¸ƒ: {[assigned_loras.count(lora) for lora in lora_requests]}")
    print(f"   æœ€å¤§ç”Ÿæˆtokens: {sampling_params.max_tokens}")
    print(f"   é‡‡æ ·ç­–ç•¥: è´ªå©ªè§£ç  (temperature=0)")
    
    # ğŸ”¥ æ–¹æ³•1ï¼šé€ä¸ªå¤„ç†ï¼ˆæµ‹é‡å•ä¸ªæ¨ç†çš„ç´¯ç§¯æ—¶é—´ï¼‰
    print(f"\nâš¡ æ–¹æ³•1: é€ä¸ªå¤„ç†ï¼ˆåŸºå‡†æ–¹æ³•ï¼‰...")
    sequential_results = []
    sequential_total_time = 0
    sequential_total_tokens = 0
    
    for i, (prompt, lora_req) in enumerate(zip(assigned_prompts, assigned_loras)):
        print(f"   å¤„ç†è¯·æ±‚ {i+1}/{len(assigned_prompts)} ä½¿ç”¨ {lora_req.lora_name}...")
        result = measure_inference_time(llm, [prompt], lora_req, sampling_params, f"Sequential-{i+1}")
        sequential_results.append(result)
        sequential_total_time += result['time']
        sequential_total_tokens += result['tokens']
    
    sequential_throughput = sequential_total_tokens / sequential_total_time if sequential_total_time > 0 else 0
    print(f"   é€ä¸ªå¤„ç†æ€»æ—¶é—´: {sequential_total_time:.4f}s")
    print(f"   é€ä¸ªå¤„ç†æ€»ååé‡: {sequential_throughput:.1f} tokens/s")
    
    # æ¸…ç†GPUçŠ¶æ€
    torch.cuda.empty_cache()
    time.sleep(1.0)  # å……åˆ†ä¼‘æ¯
    
    # ğŸš€ æ–¹æ³•2ï¼šæ‰¹å¤„ç†å¤šLoRAï¼ˆæŒ‰LoRAåˆ†ç»„æ‰¹å¤„ç†ï¼‰
    print(f"\nğŸš€ æ–¹æ³•2: æ‰¹å¤„ç†å¤šLoRAï¼ˆä¼˜åŒ–æ–¹æ³•ï¼‰...")
    
    # æŒ‰LoRAåˆ†ç»„è¯·æ±‚
    lora_groups = {}
    for prompt, lora_req in zip(assigned_prompts, assigned_loras):
        if lora_req.lora_name not in lora_groups:
            lora_groups[lora_req.lora_name] = {
                'lora_request': lora_req,
                'prompts': []
            }
        lora_groups[lora_req.lora_name]['prompts'].append(prompt)
    
    print(f"   åˆ†ç»„æƒ…å†µ:")
    for lora_name, group in lora_groups.items():
        print(f"     {lora_name}: {len(group['prompts'])} è¯·æ±‚")
    
    # æµ‹é‡æ‰¹å¤„ç†æ—¶é—´
    batch_results = []
    batch_total_time = 0
    batch_total_tokens = 0
    
    for lora_name, group in lora_groups.items():
        print(f"   æ‰¹å¤„ç† {lora_name}: {len(group['prompts'])} è¯·æ±‚...")
        result = measure_inference_time(
            llm, 
            group['prompts'], 
            group['lora_request'], 
            sampling_params, 
            f"Batch-{lora_name}"
        )
        batch_results.append(result)
        batch_total_time += result['time']
        batch_total_tokens += result['tokens']
    
    batch_throughput = batch_total_tokens / batch_total_time if batch_total_time > 0 else 0
    print(f"   æ‰¹å¤„ç†æ€»æ—¶é—´: {batch_total_time:.4f}s")
    print(f"   æ‰¹å¤„ç†æ€»ååé‡: {batch_throughput:.1f} tokens/s")
    
    # æ€§èƒ½åˆ†æ
    speedup = sequential_total_time / batch_total_time if batch_total_time > 0 else 0
    throughput_improvement = (batch_throughput - sequential_throughput) / sequential_throughput * 100 if sequential_throughput > 0 else 0
    time_saved = sequential_total_time - batch_total_time
    
    print(f"\nğŸ“Š å¤šLoRAå¹¶å‘æ€§èƒ½åˆ†æ:")
    print(f"   é€ä¸ªå¤„ç† vs æ‰¹å¤„ç†åŠ é€Ÿæ¯”: {speedup:.2f}x")
    print(f"   ååé‡æå‡: {throughput_improvement:+.1f}%")
    print(f"   æ—¶é—´èŠ‚çœ: {time_saved:.4f}s ({time_saved/sequential_total_time*100:.1f}%)")
    
    # éªŒè¯tokenæ•°é‡ä¸€è‡´æ€§
    if abs(sequential_total_tokens - batch_total_tokens) > 5:  # å…è®¸å°é‡å·®å¼‚
        print(f"âš ï¸ è­¦å‘Šï¼štokenæ•°é‡ä¸ä¸€è‡´ (Sequential: {sequential_total_tokens}, Batch: {batch_total_tokens})")
    else:
        print(f"âœ… Tokenæ•°é‡éªŒè¯é€šè¿‡ ({sequential_total_tokens} â‰ˆ {batch_total_tokens})")
    
    # ğŸ“ è¾“å‡ºç”Ÿæˆçš„å¥å­ï¼ˆä½¿ç”¨æ‰¹å¤„ç†ç»“æœï¼‰
    print(f"\nğŸ“ ç”Ÿæˆçš„å¥å­å±•ç¤º:")
    print(f"=" * 60)
    for result in batch_results:
        lora_name = result['method'].replace('Batch-', '')
        generated_texts = result.get('generated_texts', [])
        
        print(f"\nğŸ·ï¸  LoRA: {lora_name}")
        print(f"-" * 40)
        
        for i, text_info in enumerate(generated_texts):
            prompt = text_info['prompt']
            generated = text_info['generated']
            tokens = text_info['tokens']
            
            # æˆªæ–­è¿‡é•¿çš„promptå’Œgenerated textç”¨äºæ˜¾ç¤º
            prompt_display = prompt[:50] + "..." if len(prompt) > 50 else prompt
            generated_display = generated[:80] + "..." if len(generated) > 80 else generated
            
            print(f"   {i+1:2d}. è¾“å…¥: {prompt_display}")
            print(f"       è¾“å‡º: {generated_display}")
            print(f"       Tokens: {tokens}")
            print()
    
    print(f"=" * 60)
    
    return {
        'test_name': test_name,
        'num_loras': len(lora_requests),
        'num_requests': num_requests,
        'sequential_time': sequential_total_time,
        'batch_time': batch_total_time,
        'sequential_throughput': sequential_throughput,
        'batch_throughput': batch_throughput,
        'speedup': speedup,
        'throughput_improvement': throughput_improvement,
        'time_saved': time_saved,
        'sequential_tokens': sequential_total_tokens,
        'batch_tokens': batch_total_tokens,
        'sequential_results': sequential_results,
        'batch_results': batch_results
    }

def compare_fusion_performance(args):
    """å¯¹æ¯”èåˆå’Œéèåˆçš„æ€§èƒ½ï¼Œä¸“æ³¨äºå¤šLoRAå¹¶å‘åœºæ™¯"""
    print("ğŸ¯ QKV+LoRAèåˆæ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆä¸“æ³¨å¤šLoRAå¹¶å‘ï¼‰")
    print("=" * 80)
    
    model_path = args.model_path
    num_loras = args.num_loras
    num_requests = args.num_requests
    max_tokens = args.max_tokens
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    print(f"âœ… æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ”¢ LoRAæ•°é‡: {num_loras}")
    print(f"ğŸ“ è¯·æ±‚æ•°é‡: {num_requests}")
    print(f"ğŸ¯ æœ€å¤§tokens: {max_tokens}")
    
    # æŸ¥æ‰¾ç°æœ‰çš„LoRAï¼ˆä¸åˆ›å»ºå‰¯æœ¬ï¼‰
    try:
        model_dir = os.path.dirname(model_path)
        lora_paths = find_existing_loras(model_dir, num_loras)
        if not lora_paths:
            print(f"âŒ æœªæ‰¾åˆ°è¶³å¤Ÿçš„LoRA")
            return
    except Exception as e:
        print(f"âŒ æŸ¥æ‰¾LoRAå¤±è´¥: {e}")
        return
    
    # è®¾ç½®ç¯å¢ƒ
    env_config = setup_performance_environment()
    
    results = []
    
    # ç®€åŒ–çš„æµ‹è¯•é…ç½®ï¼ˆä¸“æ³¨äºèåˆvsä¼ ç»Ÿå¯¹æ¯”ï¼‰
    test_configs = [
        {
            "name": f"èåˆæ¨¡å¼-{num_loras}LoRA",
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "1",
                "VLLM_FORCE_TRITON_LORA": "1"  # ä½¿ç”¨ç¨³å®šçš„Tritonä½œä¸ºåŸºå‡†
            }
        },
        {
            "name": f"ä¼ ç»Ÿæ¨¡å¼-{num_loras}LoRA", 
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "0",
                "VLLM_FORCE_TRITON_LORA": "1"
            }
        }
    ]
    
    for config_idx, config in enumerate(test_configs):
        print(f"\nğŸ”§ æµ‹è¯•é…ç½® {config_idx+1}/{len(test_configs)}: {config['name']}")
        print("-" * 60)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        for key, value in config['env_changes'].items():
            os.environ[key] = value
            print(f"   è®¾ç½® {key} = {value}")
        
        try:
            # åˆ›å»ºLLM
            llm = create_performance_test_llm(model_path, num_loras)
            
            # åˆ›å»ºå¤šä¸ªLoRAè¯·æ±‚
            lora_requests = []
            for i, lora_path in enumerate(lora_paths):
                lora_req = LoRARequest(f"lora_{i+1}", i+1, lora_path)
                lora_requests.append(lora_req)
                print(f"   åˆ›å»º LoRA {i+1}: {lora_req.lora_name} -> {os.path.basename(lora_path)}")
            
            # æœ€ç»ˆé¢„çƒ­
            print("ğŸ”¥ ç³»ç»Ÿçº§é¢„çƒ­...")
            warmup_outputs = llm.generate(["Hello world"], SamplingParams(max_tokens=3))
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            time.sleep(1.0)
            
            # ğŸ”¥ å…³é”®ï¼šå¤šLoRAå¹¶å‘æµ‹è¯•
            result = run_concurrent_lora_benchmark(
                llm, lora_requests, num_requests, max_tokens, config['name']
            )
            
            # ä¿å­˜ç»“æœ
            results.append(result)
            
            print(f"âœ… é…ç½® '{config['name']}' æµ‹è¯•å®Œæˆ")
            
            # å½»åº•æ¸…ç†
            del llm
            torch.cuda.empty_cache()
            time.sleep(2)  # è®©GPUå……åˆ†ä¼‘æ¯
            
        except Exception as e:
            print(f"âŒ é…ç½® '{config['name']}' æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print("-" * 60)
    
    # è¾“å‡ºæœ€ç»ˆå¯¹æ¯”ç»“æœ
    if len(results) >= 2:
        print("\nğŸ† QKV+LoRAèåˆæ€§èƒ½ç»ˆæå¯¹æ¯”")
        print("=" * 80)
        
        fusion_result = results[0]
        traditional_result = results[1]
        
        print(f"ğŸ“Š æµ‹è¯•é…ç½®æ€»ç»“:")
        print(f"   LoRAæ•°é‡: {fusion_result['num_loras']}")
        print(f"   è¯·æ±‚æ•°é‡: {fusion_result['num_requests']}")
        print(f"   æœ€å¤§tokens: {max_tokens}")
        
        print(f"\nğŸ”µ èåˆæ¨¡å¼è¯¦ç»†ç»“æœ:")
        print(f"   æ‰¹å¤„ç†æ—¶é—´: {fusion_result['batch_time']:.4f}s")
        print(f"   æ‰¹å¤„ç†ååé‡: {fusion_result['batch_throughput']:.1f} tokens/s")
        print(f"   å†…éƒ¨åŠ é€Ÿæ¯”: {fusion_result['speedup']:.2f}x (é¡ºåºâ†’æ‰¹å¤„ç†)")
        print(f"   ç”Ÿæˆtokens: {fusion_result['batch_tokens']}")
        
        print(f"\nğŸŸ¢ ä¼ ç»Ÿæ¨¡å¼è¯¦ç»†ç»“æœ:")
        print(f"   æ‰¹å¤„ç†æ—¶é—´: {traditional_result['batch_time']:.4f}s")
        print(f"   æ‰¹å¤„ç†ååé‡: {traditional_result['batch_throughput']:.1f} tokens/s")
        print(f"   å†…éƒ¨åŠ é€Ÿæ¯”: {traditional_result['speedup']:.2f}x (é¡ºåºâ†’æ‰¹å¤„ç†)")
        print(f"   ç”Ÿæˆtokens: {traditional_result['batch_tokens']}")
        
        # èåˆ vs ä¼ ç»Ÿçš„æœ€ç»ˆå¯¹æ¯”
        fusion_vs_traditional_speedup = traditional_result['batch_time'] / fusion_result['batch_time']
        final_throughput_improvement = (fusion_result['batch_throughput'] - traditional_result['batch_throughput']) / traditional_result['batch_throughput'] * 100
        absolute_time_saved = traditional_result['batch_time'] - fusion_result['batch_time']
        
        print(f"\nğŸ”¥ èåˆä¼˜åŒ–æœ€ç»ˆæ•ˆæœè¯„ä¼°:")
        print(f"   èåˆæ¨¡å¼æ‰¹å¤„ç†æ—¶é—´: {fusion_result['batch_time']:.4f}s")
        print(f"   ä¼ ç»Ÿæ¨¡å¼æ‰¹å¤„ç†æ—¶é—´: {traditional_result['batch_time']:.4f}s")
        print(f"   ğŸš€ èåˆåŠ é€Ÿæ¯”: {fusion_vs_traditional_speedup:.3f}x")
        print(f"   ğŸ“ˆ ååé‡æå‡: {final_throughput_improvement:+.1f}%")
        print(f"   â±ï¸  ç»å¯¹æ—¶é—´èŠ‚çœ: {absolute_time_saved:.4f}s")
        print(f"   ğŸ“Š ç›¸å¯¹æ—¶é—´èŠ‚çœ: {absolute_time_saved/traditional_result['batch_time']*100:+.1f}%")
        
        # æ€§èƒ½è¯„ä¼°
        if fusion_vs_traditional_speedup > 1.05:
            print(f"   âœ… èåˆä¼˜åŒ–æœ‰æ•ˆï¼åŠ é€Ÿ {(fusion_vs_traditional_speedup-1)*100:.1f}%")
        elif fusion_vs_traditional_speedup > 0.95:
            print(f"   âš–ï¸  èåˆä¼˜åŒ–æ•ˆæœä¸­æ€§ (Â±5%èŒƒå›´å†…)")
        else:
            print(f"   âš ï¸  èåˆä¼˜åŒ–å‡ºç°æ€§èƒ½ä¸‹é™ {(1-fusion_vs_traditional_speedup)*100:.1f}%ï¼Œéœ€è¦è°ƒè¯•")
            
        # TokenéªŒè¯
        token_diff = abs(fusion_result['batch_tokens'] - traditional_result['batch_tokens'])
        if token_diff <= 5:
            print(f"   âœ… Tokenæ•°é‡éªŒè¯é€šè¿‡ (å·®å¼‚: {token_diff})")
        else:
            print(f"   âš ï¸ Tokenæ•°é‡å·®å¼‚è¾ƒå¤§: {token_diff}")

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("ğŸ” ç³»ç»Ÿä¿¡æ¯:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name()}")
        
        # GPUå†…å­˜ä¿¡æ¯
        gpu_props = torch.cuda.get_device_properties(0)
        total_memory = gpu_props.total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
        print(f"   GPUå†…å­˜: {allocated_memory:.1f}GB / {total_memory:.1f}GB")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="QKV+LoRAèåˆæ€§èƒ½æµ‹è¯•ï¼ˆå¤šLoRAå¹¶å‘ç‰ˆæœ¬ï¼‰"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default="/home/vllm/hf_models/Qwen2.5-1.5B",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--num-loras",
        type=int,
        default=3,
        choices=range(2, 7),  # 2-6ä¸ªLoRA
        help="å¹¶å‘LoRAæ•°é‡ (2-6)"
    )
    parser.add_argument(
        "--num-requests",
        type=int, 
        default=12,
        help="æ€»è¯·æ±‚æ•°é‡"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=20,
        help="æ¯ä¸ªè¯·æ±‚æœ€å¤§ç”Ÿæˆtokens"
    )
    
    args = parser.parse_args()
    
    print("ğŸ¯ QKV+LoRAèåˆæ€§èƒ½æµ‹è¯•ï¼ˆå¤šLoRAå¹¶å‘ä¸“ç‰ˆï¼‰")
    print("ğŸ”¥ ä¸“æ³¨äºçœŸæ­£çš„å¤šLoRAå¹¶å‘åœºæ™¯æ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    
    # æ‰“å°æµ‹è¯•å‚æ•°
    print("ğŸ® æµ‹è¯•å‚æ•°:")
    print(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"   LoRAæ•°é‡: {args.num_loras}")
    print(f"   è¯·æ±‚æ•°é‡: {args.num_requests}")
    print(f"   æœ€å¤§tokens: {args.max_tokens}")
    print()
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()
    print()
    
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”
    compare_fusion_performance(args)
    
    print("\nğŸ‰ å¤šLoRAå¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    print("ğŸ“Š æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†æ€§èƒ½æŠ¥å‘Šä»¥äº†è§£èåˆä¼˜åŒ–æ•ˆæœ")
    print("ğŸ’¡ å…³é”®æŒ‡æ ‡è§£è¯»:")
    print("   - ğŸš€ èåˆåŠ é€Ÿæ¯” > 1.05ï¼šæœ‰æ•ˆä¼˜åŒ–")
    print("   - ğŸ“ˆ ååé‡æå‡ï¼štokens/sçš„æ”¹å–„ç™¾åˆ†æ¯”")
    print("   - â±ï¸  æ—¶é—´èŠ‚çœï¼šç»å¯¹å’Œç›¸å¯¹æ—¶é—´æ”¹å–„")
    print("   - âš–ï¸  å†…éƒ¨åŠ é€Ÿæ¯”ï¼šæ‰¹å¤„ç†ç›¸æ¯”é€ä¸ªå¤„ç†çš„æ•ˆç‡")

if __name__ == "__main__":
    main() 