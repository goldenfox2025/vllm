#!/usr/bin/env python3
"""
QKV+LoRAèåˆæ€§èƒ½æ—¶é—´æµ‹è¯•è„šæœ¬
åŸºäºrun_vllm_lora_ncu.pyï¼Œä¸“é—¨æµ‹è¯•èåˆä¼˜åŒ–çš„æ—¶é—´æ€§èƒ½
æ”¯æŒçœŸæ­£çš„å¤šLoRAå¹¶å‘åœºæ™¯æµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
export VLLM_ENABLE_QKV_LORA_FUSION=1
export VLLM_ENABLE_LORA_TIMING=1
export VLLM_FORCE_TRITON_LORA=1  # å¼ºåˆ¶ä½¿ç”¨Tritonä»¥ä¾¿NCUåˆ†æ
python test_qkv_lora_timing.py
"""

import os
import sys
import time
import torch
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

def setup_performance_environment():
    """è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒå˜é‡"""
    print("ğŸ”§ è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")
    
    # æ ¸å¿ƒæ€§èƒ½ç¯å¢ƒå˜é‡
    performance_env = {
        "VLLM_ENABLE_QKV_LORA_FUSION": "1",  # å¯ç”¨QKV+LoRAèåˆ
        "VLLM_ENABLE_LORA_TIMING": "1",      # å¯ç”¨è¯¦ç»†æ—¶é—´æµ‹é‡
        "VLLM_FORCE_TRITON_LORA": "1",       # å¼ºåˆ¶ä½¿ç”¨Tritonï¼ˆä¾¿äºNCUåˆ†æï¼‰
        "VLLM_USE_V1": "0",                  # ä½¿ç”¨V0å¼•æ“
    }
    
    for key, value in performance_env.items():
        os.environ[key] = value
        print(f"   âœ… {key} = {value}")
    
    return performance_env

def create_performance_test_llm(model_path: str):
    """åˆ›å»ºä¼˜åŒ–çš„LLMå®ä¾‹ç”¨äºæ€§èƒ½æµ‹è¯•"""
    print(f"ğŸš€ åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•LLM...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    
    llm = LLM(
        model=model_path,
        enable_lora=True,
        max_lora_rank=128,           # æ”¯æŒè¾ƒå¤§çš„LoRA rank
        max_loras=2,                # ğŸ”¥ æ”¯æŒ2ä¸ªLoRAåŒæ—¶å·¥ä½œ
        max_model_len=512,          # è¾ƒå°çš„ä¸Šä¸‹æ–‡ä»¥åŠ å¿«æµ‹è¯•
        tensor_parallel_size=1,
        gpu_memory_utilization=0.7, # ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
        enforce_eager=True,          # ç¦ç”¨ç¼–è¯‘ï¼Œä¾¿äºæµ‹é‡çœŸå®æ€§èƒ½
        disable_custom_all_reduce=True,
        trust_remote_code=True,
        max_num_seqs=8,             # ğŸ”¥ æ”¯æŒæ‰¹å¤„ç†å’Œå¤šLoRA
    )
    
    print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
    return llm

def run_single_lora_benchmark(llm, lora_request, test_name: str):
    """è¿è¡Œå•ä¸ªLoRAçš„åŸºå‡†æµ‹è¯•"""
    print(f"\nâ±ï¸  å¼€å§‹å•LoRAåŸºå‡†æµ‹è¯•: {test_name}")
    print("=" * 60)
    
    # æµ‹è¯•prompts
    test_prompts = [
        "Hello, how are you today?",
        "What is the capital of France?",
        "Explain machine learning briefly.",
    ]
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.1,
        top_p=0.9,
        max_tokens=50,  # è¾ƒçŸ­ç”Ÿæˆä»¥åŠ å¿«æµ‹è¯•
    )
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   Promptsæ•°é‡: {len(test_prompts)}")
    print(f"   æœ€å¤§ç”Ÿæˆtokens: {sampling_params.max_tokens}")
    
    # æ‰¹å¤„ç†æµ‹è¯•
    print("ğŸ“¦ æ‰¹å¤„ç†æµ‹è¯•:")
    batch_start = time.perf_counter()
    batch_outputs = llm.generate(test_prompts, sampling_params, lora_request=lora_request)
    batch_end = time.perf_counter()
    
    batch_time = batch_end - batch_start
    total_tokens = sum(len(output.outputs[0].token_ids) for output in batch_outputs)
    batch_tokens_per_sec = total_tokens / batch_time if batch_time > 0 else 0
    
    print(f"   æ‰¹å¤„ç†æ—¶é—´: {batch_time:.3f}s")
    print(f"   æ€»tokens: {total_tokens}")
    print(f"   æ‰¹å¤„ç†ååé‡: {batch_tokens_per_sec:.1f} tokens/s")
    
    return {
        'test_name': test_name,
        'batch_time': batch_time,
        'batch_throughput': batch_tokens_per_sec,
        'total_tokens': total_tokens
    }

def run_mixed_lora_benchmark(llm, lora1_request, lora2_request, test_name: str):
    """è¿è¡Œæ··åˆLoRAçš„åŸºå‡†æµ‹è¯•ï¼ˆçœŸæ­£çš„å¤šLoRAå¹¶å‘ï¼‰"""
    print(f"\nğŸ”¥ å¼€å§‹æ··åˆLoRAåŸºå‡†æµ‹è¯•: {test_name}")
    print("ğŸš€ è¿™æ˜¯çœŸæ­£çš„å¤šLoRAå¹¶å‘åœºæ™¯ï¼")
    print("=" * 60)
    
    # åˆ›å»ºäº¤æ›¿çš„promptså’ŒLoRAåˆ†é…ï¼ˆæ¨¡æ‹Ÿrun_vllm_lora_ncu.pyï¼‰
    mixed_batch_prompts = [
        "Hello, how are you?",                          # LoRA 1
        "Write a short story about a robot.",           # LoRA 2
        "What is the capital of France?",               # LoRA 1
        "What are the benefits of renewable energy?",   # LoRA 2
        "Explain quantum computing briefly.",           # LoRA 1
        "Describe the process of photosynthesis.",      # LoRA 2
    ]

    mixed_batch_loras = [
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
        lora1_request,  # LoRA 1
        lora2_request,  # LoRA 2
    ]
    
    print(f"ğŸ¯ æ··åˆæ‰¹æ¬¡é…ç½®:")
    for i, (prompt, lora_req) in enumerate(zip(mixed_batch_prompts, mixed_batch_loras)):
        print(f"   [{i+1}] {lora_req.lora_name}: {prompt[:40]}...")
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.1,
        top_p=0.9,
        max_tokens=50,
    )
    
    print(f"\nğŸ”„ æ–¹æ³•1: é¡ºåºæ··åˆå¤„ç†ï¼ˆæ¨¡æ‹ŸçœŸå®æ··åˆæ‰¹æ¬¡ï¼‰...")
    sequential_start = time.perf_counter()
    mixed_outputs_sequential = []
    for i, (prompt, lora_req) in enumerate(zip(mixed_batch_prompts, mixed_batch_loras)):
        print(f"   å¤„ç†è¯·æ±‚ {i+1}/{len(mixed_batch_prompts)} ä½¿ç”¨ {lora_req.lora_name}...")
        output = llm.generate([prompt], sampling_params, lora_request=lora_req)
        mixed_outputs_sequential.extend(output)
    sequential_end = time.perf_counter()
    
    sequential_time = sequential_end - sequential_start
    sequential_tokens = sum(len(output.outputs[0].token_ids) for output in mixed_outputs_sequential)
    sequential_throughput = sequential_tokens / sequential_time if sequential_time > 0 else 0
    
    print(f"   é¡ºåºæ··åˆæ—¶é—´: {sequential_time:.3f}s")
    print(f"   é¡ºåºæ··åˆååé‡: {sequential_throughput:.1f} tokens/s")
    
    print(f"\nğŸš€ æ–¹æ³•2: å¹¶å‘æ‰¹å¤„ç†ï¼ˆåˆ†ç¦»LoRAæ‰¹æ¬¡ï¼‰...")
    # åˆ†ç¦»ä¸åŒLoRAçš„prompts
    lora1_batch_prompts = [mixed_batch_prompts[i] for i in range(0, len(mixed_batch_prompts), 2)]
    lora2_batch_prompts = [mixed_batch_prompts[i] for i in range(1, len(mixed_batch_prompts), 2)]
    
    print(f"   LoRA 1 æ‰¹æ¬¡: {len(lora1_batch_prompts)} prompts")
    print(f"   LoRA 2 æ‰¹æ¬¡: {len(lora2_batch_prompts)} prompts")
    
    concurrent_start = time.perf_counter()
    
    # å¹¶å‘å¤„ç†ä¸¤ä¸ªLoRAæ‰¹æ¬¡
    lora1_batch_outputs = llm.generate(lora1_batch_prompts, sampling_params, lora_request=lora1_request)
    lora2_batch_outputs = llm.generate(lora2_batch_prompts, sampling_params, lora_request=lora2_request)
    
    concurrent_end = time.perf_counter()
    
    # äº¤é”™ç»“æœä»¥åŒ¹é…åŸå§‹é¡ºåº
    mixed_outputs_concurrent = []
    for i in range(max(len(lora1_batch_outputs), len(lora2_batch_outputs))):
        if i < len(lora1_batch_outputs):
            mixed_outputs_concurrent.append(lora1_batch_outputs[i])
        if i < len(lora2_batch_outputs):
            mixed_outputs_concurrent.append(lora2_batch_outputs[i])
    
    concurrent_time = concurrent_end - concurrent_start
    concurrent_tokens = sum(len(output.outputs[0].token_ids) for output in mixed_outputs_concurrent)
    concurrent_throughput = concurrent_tokens / concurrent_time if concurrent_time > 0 else 0
    
    print(f"   å¹¶å‘æ‰¹å¤„ç†æ—¶é—´: {concurrent_time:.3f}s")
    print(f"   å¹¶å‘æ‰¹å¤„ç†ååé‡: {concurrent_throughput:.1f} tokens/s")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ··åˆLoRAæ€§èƒ½å¯¹æ¯”:")
    print(f"   é¡ºåºå¤„ç† vs å¹¶å‘å¤„ç†åŠ é€Ÿæ¯”: {sequential_time / concurrent_time:.2f}x")
    print(f"   ååé‡æå‡: {(concurrent_throughput - sequential_throughput) / sequential_throughput * 100:.1f}%")
    
    return {
        'test_name': test_name,
        'sequential_time': sequential_time,
        'concurrent_time': concurrent_time,
        'sequential_throughput': sequential_throughput,
        'concurrent_throughput': concurrent_throughput,
        'speedup': sequential_time / concurrent_time,
        'total_tokens': concurrent_tokens
    }

def compare_fusion_performance():
    """å¯¹æ¯”èåˆå’Œéèåˆçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤šLoRAåœºæ™¯"""
    print("ğŸ¯ QKV+LoRAèåˆæ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆå¤šLoRAå¹¶å‘ï¼‰")
    print("=" * 80)
    
    model_path = "/home/vllm/hf_models/Qwen2.5-1.5B"
    lora1_path = "/home/vllm/hf_models/Qwen2.5-1.5B-lora1"
    lora2_path = "/home/vllm/hf_models/Qwen2.5-1.5B-lora2"
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    if not os.path.exists(lora1_path):
        print(f"âŒ LoRA 1è·¯å¾„ä¸å­˜åœ¨: {lora1_path}")
        return
        
    if not os.path.exists(lora2_path):
        print(f"âŒ LoRA 2è·¯å¾„ä¸å­˜åœ¨: {lora2_path}")
        print("ğŸ’¡ æç¤ºï¼šéœ€è¦åˆ›å»ºç¬¬äºŒä¸ªLoRA")
        # åˆ›å»ºlora2ï¼ˆå¤åˆ¶lora1ï¼‰
        import shutil
        if os.path.exists(lora1_path):
            print(f"ğŸ”§ è‡ªåŠ¨å¤åˆ¶LoRA 1åˆ°LoRA 2...")
            shutil.copytree(lora1_path, lora2_path)
            print(f"âœ… åˆ›å»ºLoRA 2å®Œæˆ: {lora2_path}")
        else:
            return
    
    print(f"âœ… æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"âœ… LoRA 1è·¯å¾„: {lora1_path}")
    print(f"âœ… LoRA 2è·¯å¾„: {lora2_path}")
    
    # è®¾ç½®ç¯å¢ƒ
    env_config = setup_performance_environment()
    
    results = []
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            "name": "èåˆæ¨¡å¼ + Triton + å¤šLoRA",
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "1",
                "VLLM_FORCE_TRITON_LORA": "1"
            }
        },
        {
            "name": "èåˆæ¨¡å¼ + CUDA + å¤šLoRA",
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "1", 
                "VLLM_FORCE_TRITON_LORA": "0"
            }
        },
        {
            "name": "ä¼ ç»Ÿæ¨¡å¼ + Triton + å¤šLoRA",
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "0",
                "VLLM_FORCE_TRITON_LORA": "1"
            }
        }
    ]
    
    for config in test_configs:
        print(f"\nğŸ”§ æµ‹è¯•é…ç½®: {config['name']}")
        print("-" * 60)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        for key, value in config['env_changes'].items():
            os.environ[key] = value
            print(f"   è®¾ç½® {key} = {value}")
        
        try:
            # åˆ›å»ºLLM
            llm = create_performance_test_llm(model_path)
            
            # åˆ›å»ºä¸¤ä¸ªLoRAè¯·æ±‚
            lora1_request = LoRARequest("test_lora1", 1, lora1_path)
            lora2_request = LoRARequest("test_lora2", 2, lora2_path)
            
            print(f"âœ… åˆ›å»ºLoRAè¯·æ±‚:")
            print(f"   LoRA 1: {lora1_request}")
            print(f"   LoRA 2: {lora2_request}")
            
            # é¢„çƒ­
            print("ğŸ”¥ é¢„çƒ­é˜¶æ®µ...")
            warmup_outputs = llm.generate(["Hello"], SamplingParams(max_tokens=10))
            
            # å•LoRAæµ‹è¯•
            print("\nğŸ“Š å•LoRAæ€§èƒ½æµ‹è¯•...")
            lora1_result = run_single_lora_benchmark(llm, lora1_request, f"{config['name']} - LoRA1")
            lora2_result = run_single_lora_benchmark(llm, lora2_request, f"{config['name']} - LoRA2")
            
            # ğŸ”¥ å…³é”®ï¼šæ··åˆLoRAæµ‹è¯•ï¼ˆçœŸæ­£çš„å¤šLoRAå¹¶å‘ï¼‰
            mixed_result = run_mixed_lora_benchmark(llm, lora1_request, lora2_request, config['name'])
            
            # ä¿å­˜ç»“æœ
            config_result = {
                'config_name': config['name'],
                'lora1_result': lora1_result,
                'lora2_result': lora2_result,
                'mixed_result': mixed_result
            }
            results.append(config_result)
            
            print(f"âœ… é…ç½® '{config['name']}' æµ‹è¯•å®Œæˆ")
            
            # æ¸…ç†
            del llm
            torch.cuda.empty_cache()
            time.sleep(1)  # è®©GPUç¨å¾®ä¼‘æ¯
            
        except Exception as e:
            print(f"âŒ é…ç½® '{config['name']}' æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print("-" * 60)
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    if len(results) >= 2:
        print("\nğŸ† å¤šLoRAæ€§èƒ½å¯¹æ¯”ç»“æœ")
        print("=" * 80)
        
        print("ğŸ“Š å„é…ç½®æ··åˆLoRAå¹¶å‘æ€§èƒ½:")
        for i, result in enumerate(results):
            mixed = result['mixed_result']
            print(f"   {i+1}. {result['config_name']}:")
            print(f"      å¹¶å‘å¤„ç†æ—¶é—´: {mixed['concurrent_time']:.3f}s")
            print(f"      å¹¶å‘ååé‡: {mixed['concurrent_throughput']:.1f} tokens/s")
            print(f"      å†…éƒ¨åŠ é€Ÿæ¯”: {mixed['speedup']:.2f}x (é¡ºåºvså¹¶å‘)")
        
        # èåˆ vs ä¼ ç»Ÿå¯¹æ¯”
        if len(results) >= 3:
            fusion_result = results[0]['mixed_result']  # èåˆæ¨¡å¼
            traditional_result = results[2]['mixed_result']  # ä¼ ç»Ÿæ¨¡å¼
            
            fusion_vs_traditional_speedup = traditional_result['concurrent_time'] / fusion_result['concurrent_time']
            throughput_improvement = (fusion_result['concurrent_throughput'] - traditional_result['concurrent_throughput']) / traditional_result['concurrent_throughput'] * 100
            
            print(f"\nğŸ”¥ èåˆæ¨¡å¼ vs ä¼ ç»Ÿæ¨¡å¼ï¼ˆå¤šLoRAå¹¶å‘ï¼‰:")
            print(f"   èåˆæ¨¡å¼æ—¶é—´: {fusion_result['concurrent_time']:.3f}s")
            print(f"   ä¼ ç»Ÿæ¨¡å¼æ—¶é—´: {traditional_result['concurrent_time']:.3f}s")
            print(f"   èåˆåŠ é€Ÿæ¯”: {fusion_vs_traditional_speedup:.2f}x")
            print(f"   ååé‡æå‡: {throughput_improvement:+.1f}%")

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("ğŸ” ç³»ç»Ÿä¿¡æ¯:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name()}")
        
        # GPUå†…å­˜ä¿¡æ¯
        gpu_props = torch.cuda.get_device_properties(0)
        total_memory = gpu_props.total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
        print(f"   GPUå†…å­˜: {allocated_memory:.1f}GB / {total_memory:.1f}GB")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ QKV+LoRAèåˆæ€§èƒ½æ—¶é—´æµ‹è¯•ï¼ˆçœŸæ­£çš„å¤šLoRAå¹¶å‘ï¼‰")
    print("ğŸ”¥ æ¨¡æ‹Ÿrun_vllm_lora_ncu.pyçš„å¤šLoRAæ··åˆæ‰¹å¤„ç†åœºæ™¯")
    print("=" * 80)
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()
    print()
    
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”
    compare_fusion_performance()
    
    print("\nğŸ‰ å¤šLoRAå¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    print("ğŸ“Š æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†æ€§èƒ½æŠ¥å‘Šä»¥äº†è§£èåˆä¼˜åŒ–åœ¨å¤šLoRAåœºæ™¯ä¸‹çš„æ•ˆæœ")
    print("ğŸ’¡ æç¤ºï¼š")
    print("   - èåˆæ¨¡å¼åº”è¯¥åœ¨å¤šLoRAå¹¶å‘æ—¶æ˜¾ç¤ºæ›´å¥½æ€§èƒ½")
    print("   - æ··åˆLoRAæ‰¹å¤„ç†æµ‹è¯•äº†çœŸæ­£çš„å¤šLoRAå†…æ ¸èåˆ")
    print("   - å¯ä»¥ä½¿ç”¨ nsys æˆ– ncu è¿›è¡Œæ›´æ·±å…¥çš„å¤šLoRAå†…æ ¸åˆ†æ")

if __name__ == "__main__":
    main() 