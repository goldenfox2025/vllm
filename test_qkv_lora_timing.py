#!/usr/bin/env python3
"""
QKV+LoRAèåˆæ­£ç¡®æ€§å’Œæ€§èƒ½æµ‹è¯•è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆ - ä¸“æ³¨æ··åˆLoRAï¼‰
æµ‹è¯•å¤šä¸ªLoRAåœ¨åŒä¸€æ‰¹æ¬¡ä¸­çš„å¤„ç†ï¼ŒéªŒè¯èåˆè®¡ç®—çš„æ­£ç¡®æ€§å’Œæ€§èƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
export VLLM_ENABLE_QKV_LORA_FUSION=1
export VLLM_ENABLE_LORA_TIMING=1
python test_qkv_lora_timing.py --num-loras 3 --batch-size 6
"""

import argparse
import os
import sys
import time
import torch
import random
import shutil
import glob
from pathlib import Path
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

def setup_performance_environment():
    """è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒå˜é‡"""
    print("ğŸ”§ è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")
    
    # æ ¸å¿ƒæ€§èƒ½ç¯å¢ƒå˜é‡
    performance_env = {
        "VLLM_ENABLE_QKV_LORA_FUSION": "1",  # å¯ç”¨QKV+LoRAèåˆ
        "VLLM_ENABLE_LORA_TIMING": "1",      # å¯ç”¨è¯¦ç»†æ—¶é—´æµ‹é‡
        "VLLM_USE_V1": "0",                  # ä½¿ç”¨V0å¼•æ“
    }
    
    for key, value in performance_env.items():
        os.environ[key] = value
        print(f"   âœ… {key} = {value}")
    
    return performance_env

def find_existing_loras(model_dir: str, num_loras: int) -> list[str]:
    """ä»æ¨¡å‹ç›®å½•ä¸­æŸ¥æ‰¾ç°æœ‰çš„LoRA"""
    print(f"ğŸ” åœ¨ {model_dir} ä¸­æŸ¥æ‰¾ç°æœ‰çš„ {num_loras} ä¸ªLoRA...")
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„LoRAç›®å½•
    possible_patterns = [
        "*lora*",
        "*LoRA*", 
        "*LORA*"
    ]
    
    found_loras = []
    for pattern in possible_patterns:
        lora_paths = glob.glob(os.path.join(model_dir, pattern))
        for path in lora_paths:
            if os.path.isdir(path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„LoRAç›®å½•ï¼ˆåŒ…å«adapter_config.jsonï¼‰
                if os.path.exists(os.path.join(path, "adapter_config.json")):
                    found_loras.append(path)
    
    # å»é‡å¹¶æ’åº
    found_loras = sorted(list(set(found_loras)))
    
    print(f"ğŸ” æ‰¾åˆ° {len(found_loras)} ä¸ªLoRA:")
    for i, lora_path in enumerate(found_loras):
        print(f"   {i+1}. {os.path.basename(lora_path)}: {lora_path}")
    
    if len(found_loras) < num_loras:
        print(f"âš ï¸ åªæ‰¾åˆ° {len(found_loras)} ä¸ªLoRAï¼Œä½†éœ€è¦ {num_loras} ä¸ª")
        print(f"ğŸ’¡ å°†é‡å¤ä½¿ç”¨ç°æœ‰LoRAä»¥è¾¾åˆ°æ‰€éœ€æ•°é‡")
        
        # é‡å¤ä½¿ç”¨ç°æœ‰LoRAç›´åˆ°è¾¾åˆ°æ‰€éœ€æ•°é‡
        while len(found_loras) < num_loras:
            found_loras.extend(found_loras[:min(len(found_loras), num_loras - len(found_loras))])
    
    # åªè¿”å›æ‰€éœ€æ•°é‡çš„LoRA
    selected_loras = found_loras[:num_loras]
    
    print(f"âœ… æœ€ç»ˆé€‰æ‹©çš„ {len(selected_loras)} ä¸ªLoRA:")
    for i, lora_path in enumerate(selected_loras):
        print(f"   {i+1}. {os.path.basename(lora_path)}")
    
    return selected_loras

def create_test_llm(model_path: str, max_loras: int):
    """åˆ›å»ºæµ‹è¯•LLMå®ä¾‹"""
    print(f"ğŸš€ åˆå§‹åŒ–æµ‹è¯•LLM (æ”¯æŒ{max_loras}ä¸ªLoRA)...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    
    llm = LLM(
        model=model_path,
        enable_lora=True,
        max_lora_rank=128,           
        max_loras=max_loras,         
        max_model_len=256,           
        tensor_parallel_size=1,
        gpu_memory_utilization=0.65, 
        enforce_eager=True,          
        disable_custom_all_reduce=True,
        trust_remote_code=True,
        max_num_seqs=16,             
    )
    
    print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
    return llm

def generate_mixed_batch(num_loras: int, batch_size: int) -> tuple[list[str], list[int]]:
    """ç”Ÿæˆæ··åˆLoRAæ‰¹æ¬¡çš„promptså’Œloraåˆ†é…"""
    base_prompts = [
        "Hello, how are you?",
        "What is AI?",
        "Explain quantum physics.",
        "Write a poem about stars.",
        "How does blockchain work?",
        "Describe machine learning.",
        "What is the future of technology?",
        "How do neural networks learn?",
    ]
    
    prompts = []
    lora_assignments = []
    
    for i in range(batch_size):
        # è½®å¾ªåˆ†é…LoRA
        lora_id = (i % num_loras) + 1  # LoRA IDä»1å¼€å§‹
        prompt = base_prompts[i % len(base_prompts)]
        
        # æ·»åŠ åºå·è®©æ¯ä¸ªpromptä¸åŒ
        prompt = f"[{i+1}] {prompt}"
        
        prompts.append(prompt)
        lora_assignments.append(lora_id)
    
    return prompts, lora_assignments

def test_mixed_lora_batch(llm, lora_requests: list[LoRARequest], batch_size: int) -> dict:
    """æµ‹è¯•æ··åˆLoRAæ‰¹æ¬¡å¤„ç†"""
    print(f"\nğŸ”¥ æµ‹è¯•æ··åˆLoRAæ‰¹æ¬¡å¤„ç†")
    print(f"ğŸ¯ LoRAæ•°é‡: {len(lora_requests)}, æ‰¹æ¬¡å¤§å°: {batch_size}")
    print("=" * 60)
    
    # ç”Ÿæˆæ··åˆæ‰¹æ¬¡
    prompts, lora_assignments = generate_mixed_batch(len(lora_requests), batch_size)
    
    # æ‰“å°æ‰¹æ¬¡åˆ†é…
    print(f"ğŸ“ æ··åˆæ‰¹æ¬¡åˆ†é…:")
    for i, (prompt, lora_id) in enumerate(zip(prompts, lora_assignments)):
        lora_name = lora_requests[lora_id-1].lora_name
        print(f"   [{i+1:2d}] LoRA-{lora_id}({lora_name}): {prompt}")
    
    # é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.0,  # è´ªå©ªè§£ç 
        max_tokens=15,    # è¾ƒçŸ­çš„è¾“å‡ºä¾¿äºå¿«é€Ÿæµ‹è¯•
    )
    
    # é¢„çƒ­
    print(f"\nğŸ”¥ é¢„çƒ­...")
    warmup_outputs = llm.generate([prompts[0]], sampling_params)
    torch.cuda.synchronize()
    torch.cuda.empty_cache()
    time.sleep(0.5)
    
    # æ­£å¼æµ‹è¯•
    print(f"\nâš¡ å¼€å§‹æ··åˆLoRAæ‰¹æ¬¡æ¨ç†...")
    
    start_time = time.perf_counter()
    torch.cuda.synchronize()
    
    # è¿™é‡ŒvLLMä¼šè‡ªåŠ¨å¤„ç†æ··åˆLoRAæ‰¹æ¬¡
    outputs = llm.generate(prompts, sampling_params)
    
    torch.cuda.synchronize()
    end_time = time.perf_counter()
    
    inference_time = end_time - start_time
    total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)
    throughput = total_tokens / inference_time if inference_time > 0 else 0
    
    print(f"   âœ… æ··åˆæ‰¹æ¬¡æ¨ç†å®Œæˆ")
    print(f"   ğŸ“Š æ¨ç†æ—¶é—´: {inference_time:.4f}s")
    print(f"   ğŸ“Š æ€»tokens: {total_tokens}")
    print(f"   ğŸ“Š ååé‡: {throughput:.1f} tokens/s")
    
    # æ˜¾ç¤ºç”Ÿæˆç»“æœ
    print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
    print(f"-" * 60)
    for i, (output, lora_id) in enumerate(zip(outputs, lora_assignments)):
        prompt = prompts[i]
        generated = output.outputs[0].text
        tokens = len(output.outputs[0].token_ids)
        lora_name = lora_requests[lora_id-1].lora_name
        
        print(f"[{i+1:2d}] LoRA-{lora_id}({lora_name}):")
        print(f"     è¾“å…¥: {prompt}")
        print(f"     è¾“å‡º: {generated}")
        print(f"     Tokens: {tokens}")
        print()
    
    return {
        'inference_time': inference_time,
        'total_tokens': total_tokens,
        'throughput': throughput,
        'batch_size': batch_size,
        'num_loras': len(lora_requests),
        'outputs': outputs,
        'prompts': prompts,
        'lora_assignments': lora_assignments
    }

def compare_fusion_vs_traditional(args):
    """å¯¹æ¯”èåˆå’Œä¼ ç»Ÿæ–¹æ³•åœ¨æ··åˆLoRAåœºæ™¯ä¸‹çš„æ€§èƒ½"""
    print("ğŸ¯ QKV+LoRAèåˆ vs ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”æµ‹è¯•ï¼ˆæ··åˆLoRAæ‰¹æ¬¡ï¼‰")
    print("=" * 80)
    
    model_path = args.model_path
    num_loras = args.num_loras
    batch_size = args.batch_size
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    print(f"âœ… æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ”¢ LoRAæ•°é‡: {num_loras}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # æŸ¥æ‰¾LoRA
    try:
        model_dir = os.path.dirname(model_path)
        lora_paths = find_existing_loras(model_dir, num_loras)
        if not lora_paths:
            print(f"âŒ æœªæ‰¾åˆ°è¶³å¤Ÿçš„LoRA")
            return
    except Exception as e:
        print(f"âŒ æŸ¥æ‰¾LoRAå¤±è´¥: {e}")
        return
    
    # è®¾ç½®ç¯å¢ƒ
    setup_performance_environment()
    
    results = []
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            "name": "èåˆæ¨¡å¼",
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "1",
                "VLLM_FORCE_TRITON_LORA": "1"
            }
        },
        {
            "name": "ä¼ ç»Ÿæ¨¡å¼", 
            "env_changes": {
                "VLLM_ENABLE_QKV_LORA_FUSION": "0",
                "VLLM_FORCE_TRITON_LORA": "1"
            }
        }
    ]
    
    for config_idx, config in enumerate(test_configs):
        print(f"\nğŸ”§ æµ‹è¯•é…ç½® {config_idx+1}/{len(test_configs)}: {config['name']}")
        print("-" * 60)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        for key, value in config['env_changes'].items():
            os.environ[key] = value
            print(f"   è®¾ç½® {key} = {value}")
        
        try:
            # åˆ›å»ºLLM
            llm = create_test_llm(model_path, num_loras)
            
            # åˆ›å»ºLoRAè¯·æ±‚
            lora_requests = []
            for i, lora_path in enumerate(lora_paths):
                lora_req = LoRARequest(f"lora_{i+1}", i+1, lora_path)
                lora_requests.append(lora_req)
                print(f"   åˆ›å»º LoRA {i+1}: {lora_req.lora_name} -> {os.path.basename(lora_path)}")
            
            # è¿è¡Œæ··åˆLoRAæµ‹è¯•
            result = test_mixed_lora_batch(llm, lora_requests, batch_size)
            result['config_name'] = config['name']
            results.append(result)
            
            print(f"âœ… é…ç½® '{config['name']}' æµ‹è¯•å®Œæˆ")
            
            # æ¸…ç†
            del llm
            torch.cuda.empty_cache()
            time.sleep(2)
            
        except Exception as e:
            print(f"âŒ é…ç½® '{config['name']}' æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print("-" * 60)
    
    # æœ€ç»ˆå¯¹æ¯”ç»“æœ
    if len(results) >= 2:
        print("\nğŸ† èåˆ vs ä¼ ç»Ÿæ–¹æ³•æœ€ç»ˆå¯¹æ¯”")
        print("=" * 80)
        
        fusion_result = results[0]
        traditional_result = results[1]
        
        print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
        print(f"   LoRAæ•°é‡: {fusion_result['num_loras']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {fusion_result['batch_size']}")
        print(f"   æ¯LoRAå¹³å‡è¯·æ±‚: {fusion_result['batch_size'] / fusion_result['num_loras']:.1f}")
        
        print(f"\nğŸ”µ èåˆæ¨¡å¼ç»“æœ:")
        print(f"   æ¨ç†æ—¶é—´: {fusion_result['inference_time']:.4f}s")
        print(f"   ååé‡: {fusion_result['throughput']:.1f} tokens/s")
        print(f"   ç”Ÿæˆtokens: {fusion_result['total_tokens']}")
        
        print(f"\nğŸŸ¢ ä¼ ç»Ÿæ¨¡å¼ç»“æœ:")
        print(f"   æ¨ç†æ—¶é—´: {traditional_result['inference_time']:.4f}s")
        print(f"   ååé‡: {traditional_result['throughput']:.1f} tokens/s")
        print(f"   ç”Ÿæˆtokens: {traditional_result['total_tokens']}")
        
        # æ€§èƒ½å¯¹æ¯”
        if traditional_result['inference_time'] > 0:
            speedup = traditional_result['inference_time'] / fusion_result['inference_time']
            throughput_improvement = (fusion_result['throughput'] - traditional_result['throughput']) / traditional_result['throughput'] * 100
            time_saved = traditional_result['inference_time'] - fusion_result['inference_time']
            
            print(f"\nğŸš€ èåˆä¼˜åŒ–æ•ˆæœ:")
            print(f"   åŠ é€Ÿæ¯”: {speedup:.3f}x")
            print(f"   ååé‡æå‡: {throughput_improvement:+.1f}%")
            print(f"   æ—¶é—´èŠ‚çœ: {time_saved:.4f}s ({time_saved/traditional_result['inference_time']*100:+.1f}%)")
            
            # è¯„ä¼°ç»“æœ
            if speedup > 1.05:
                print(f"   âœ… èåˆä¼˜åŒ–æœ‰æ•ˆï¼æ€§èƒ½æå‡ {(speedup-1)*100:.1f}%")
            elif speedup > 0.95:
                print(f"   âš–ï¸  èåˆä¼˜åŒ–æ•ˆæœä¸­æ€§ (Â±5%èŒƒå›´å†…)")
            else:
                print(f"   âš ï¸  èåˆä¼˜åŒ–å‡ºç°æ€§èƒ½ä¸‹é™ {(1-speedup)*100:.1f}%")
        
        # TokenéªŒè¯
        token_diff = abs(fusion_result['total_tokens'] - traditional_result['total_tokens'])
        if token_diff <= 3:
            print(f"   âœ… Tokenæ•°é‡éªŒè¯é€šè¿‡ (å·®å¼‚: {token_diff})")
        else:
            print(f"   âš ï¸ Tokenæ•°é‡å·®å¼‚: {token_diff}")
    
    print("=" * 80)

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("ğŸ” ç³»ç»Ÿä¿¡æ¯:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name()}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="QKV+LoRAèåˆæµ‹è¯•ï¼ˆä¸“æ³¨æ··åˆLoRAæ‰¹æ¬¡ï¼‰"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default="/home/vllm/hf_models/Qwen2.5-1.5B",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--num-loras",
        type=int,
        default=3,
        choices=range(2, 7),
        help="LoRAæ•°é‡ (2-6)"
    )
    parser.add_argument(
        "--batch-size",
        type=int, 
        default=6,
        help="æ··åˆæ‰¹æ¬¡å¤§å°"
    )
    
    args = parser.parse_args()
    
    print("ğŸ¯ QKV+LoRAèåˆæµ‹è¯•ï¼ˆä¸“æ³¨æ··åˆLoRAæ‰¹æ¬¡ï¼‰")
    print("ğŸ”¥ æµ‹è¯•å¤šä¸ªLoRAåœ¨åŒä¸€æ‰¹æ¬¡ä¸­çš„å¤„ç†æ­£ç¡®æ€§å’Œæ€§èƒ½")
    print("=" * 80)
    
    # æ‰“å°æµ‹è¯•å‚æ•°
    print("ğŸ® æµ‹è¯•å‚æ•°:")
    print(f"   æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"   LoRAæ•°é‡: {args.num_loras}")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print()
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()
    print()
    
    # è¿è¡Œæµ‹è¯•
    compare_fusion_vs_traditional(args)
    
    print("\nğŸ‰ æ··åˆLoRAæ‰¹æ¬¡æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ è¿™ä¸ªæµ‹è¯•ä¸“æ³¨äºéªŒè¯å¤šä¸ªLoRAåœ¨åŒä¸€æ‰¹æ¬¡ä¸­çš„æ­£ç¡®æ€§å’Œæ€§èƒ½")

if __name__ == "__main__":
    main() 