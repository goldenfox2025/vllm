#!/usr/bin/env python3
"""
ç»ˆæèåˆå†…æ ¸æµ‹è¯•è„šæœ¬
"""
import os
import torch
import sys

# è®¾ç½®ç¯å¢ƒå˜é‡
# os.environ["VLLM_ENABLE_ULTIMATE_FUSION"] = "1"
os.environ["VLLM_ENABLE_TIMING"] = "1"

def test_ultimate_fusion_standalone():
    """ç‹¬ç«‹æµ‹è¯•ç»ˆæèåˆå†…æ ¸"""
    print("ğŸ§ª ç‹¬ç«‹æµ‹è¯•ç»ˆæèåˆå†…æ ¸...")
    
    try:
        from vllm.lora.punica_wrapper.cuda_punica.ultimate_fusion_ctypes_wrapper import test_ultimate_fusion
        
        success = test_ultimate_fusion()
        if success:
            print("âœ… ç‹¬ç«‹æµ‹è¯•é€šè¿‡!")
        else:
            print("âŒ ç‹¬ç«‹æµ‹è¯•å¤±è´¥!")
        return success
        
    except Exception as e:
        print(f"âŒ ç‹¬ç«‹æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_ultimate_fusion_realistic():
    """ä½¿ç”¨æ›´çœŸå®çš„å‚æ•°æµ‹è¯•ç»ˆæèåˆå†…æ ¸"""
    print("\nğŸ§ª çœŸå®å‚æ•°æµ‹è¯•ç»ˆæèåˆå†…æ ¸...")
    
    try:
        from vllm.lora.punica_wrapper.cuda_punica.ultimate_fusion_ctypes_wrapper import cuda_ultimate_fusion_interface
        
        # æ¨¡æ‹ŸçœŸå®çš„VLLMå‚æ•°
        num_tokens = 8
        hidden_size = 1536  # çœŸå®æ¨¡å‹çš„hidden_size
        q_size = 1536      # Q projection size
        k_size = 256       # K projection size  
        v_size = 256       # V projection size
        qkv_output_size = q_size + k_size + v_size  # 2048
        rank = 16
        num_slices = 3  # Q, K, V
        
        device = torch.device('cuda:0')
        dtype = torch.float16
        
        print(f"ğŸ“Š çœŸå®å‚æ•°: hidden_size={hidden_size}, qkv_output_size={qkv_output_size}")
        print(f"   Q_size={q_size}, K_size={k_size}, V_size={v_size}")
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        inputs = torch.randn(num_tokens, hidden_size, dtype=dtype, device=device)
        qkv_weights = torch.randn(qkv_output_size, hidden_size, dtype=dtype, device=device)
        
        # åˆ›å»ºLoRAæƒé‡ (æ¯ä¸ªsliceä¸€ä¸ª)
        lora_a_stacked = tuple([
            torch.randn(1, rank, hidden_size, dtype=dtype, device=device),  # Q
            torch.randn(1, rank, hidden_size, dtype=dtype, device=device),  # K  
            torch.randn(1, rank, hidden_size, dtype=dtype, device=device),  # V
        ])
        lora_b_stacked = tuple([
            torch.randn(1, q_size, rank, dtype=dtype, device=device),  # Q
            torch.randn(1, k_size, rank, dtype=dtype, device=device),  # K
            torch.randn(1, v_size, rank, dtype=dtype, device=device),  # V
        ])
        
        output_slices = (q_size, k_size, v_size)  # Q, K, Vå„è‡ªçš„å¤§å°
        
        # åˆ›å»ºç®€å•çš„Punicaå…ƒæ•°æ®ï¼ˆæ‰€æœ‰tokenä½¿ç”¨åŒä¸€ä¸ªLoRAï¼‰
        token_indices_sorted = torch.arange(num_tokens, dtype=torch.int32, device=device)
        num_tokens_per_lora = torch.tensor([num_tokens], dtype=torch.int32, device=device)
        lora_token_start_loc = torch.tensor([0, num_tokens], dtype=torch.int32, device=device)
        lora_ids = torch.tensor([0], dtype=torch.int32, device=device)
        
        # è°ƒç”¨ç»ˆæèåˆå†…æ ¸
        output = cuda_ultimate_fusion_interface(
            inputs, qkv_weights, lora_a_stacked, lora_b_stacked, output_slices,
            token_indices_sorted, num_tokens_per_lora, lora_token_start_loc, lora_ids
        )
        
        print(f"âœ… çœŸå®å‚æ•°æµ‹è¯•é€šè¿‡!")
        print(f"ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"ğŸ“ˆ è¾“å‡ºç»Ÿè®¡: min={output.min():.3f}, max={output.max():.3f}")
        
        # æµ‹è¯•biasæ·»åŠ 
        bias = torch.randn(qkv_output_size, dtype=dtype, device=device)
        output_with_bias = output + bias.unsqueeze(0)  # å¹¿æ’­bias
        print(f"ğŸ“Œ å¸¦biasè¾“å‡ºå½¢çŠ¶: {output_with_bias.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çœŸå®å‚æ•°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ultimate_fusion_in_layers():
    """åœ¨layers.pyä¸­æµ‹è¯•ç»ˆæèåˆå†…æ ¸"""
    print("\nğŸ§ª åœ¨MergedQKVParallelLinearWithLoRAä¸­æµ‹è¯•ç»ˆæèåˆå†…æ ¸...")
    
    try:
        # åˆ›å»ºç®€å•çš„æµ‹è¯•åœºæ™¯
        # æ³¨æ„ï¼šè¿™éœ€è¦å®Œæ•´çš„VLLMç¯å¢ƒæ‰èƒ½è¿è¡Œ
        print("âš ï¸  è¿™éœ€è¦å®Œæ•´çš„VLLMç¯å¢ƒå’Œæ¨¡å‹åŠ è½½ï¼Œæš‚æ—¶è·³è¿‡...")
        return True
        
    except Exception as e:
        print(f"âŒ Layersæµ‹è¯•å¼‚å¸¸: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æµ‹è¯•ç»ˆæèåˆå†…æ ¸å®Œæ•´æµç¨‹...")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæµ‹è¯•è·³è¿‡")
        return False
    
    # æµ‹è¯•1: ç‹¬ç«‹æ¥å£æµ‹è¯•
    test1_success = test_ultimate_fusion_standalone()
    
    # æµ‹è¯•2: çœŸå®å‚æ•°æµ‹è¯•
    test2_success = test_ultimate_fusion_realistic()
    
    # æµ‹è¯•3: åœ¨layersä¸­çš„é›†æˆæµ‹è¯•
    test3_success = test_ultimate_fusion_in_layers()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"  ç‹¬ç«‹æ¥å£æµ‹è¯•: {'âœ… é€šè¿‡' if test1_success else 'âŒ å¤±è´¥'}")
    print(f"  çœŸå®å‚æ•°æµ‹è¯•: {'âœ… é€šè¿‡' if test2_success else 'âŒ å¤±è´¥'}")
    print(f"  Layersé›†æˆæµ‹è¯•: {'âœ… é€šè¿‡' if test3_success else 'âŒ å¤±è´¥'}")
    
    overall_success = test1_success and test2_success and test3_success
    
    if overall_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ç»ˆæèåˆå†…æ ¸å¯ä»¥ä½¿ç”¨!")
        print("\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
        print("   export VLLM_ENABLE_ULTIMATE_FUSION=1")
        print("   export VLLM_ENABLE_TIMING=1  # å¯é€‰ï¼Œå¯ç”¨è®¡æ—¶")
        print("\nğŸŒŸ ç»ˆæèåˆå†…æ ¸ç‰¹æ€§:")
        print("   âœ¨ ä¸€ä¸ªå†…æ ¸å®Œæˆæ‰€æœ‰è®¡ç®— (QKV + LoRA)")
        print("   ğŸš€ é›¶ç©ºç®—: ä¸ä½¿ç”¨LoRAçš„tokenä¸æµªè´¹è®¡ç®—")
        print("   ğŸ’¾ æ›´å¥½çš„ç¼“å­˜åˆ©ç”¨ç‡")
        print("   âš¡ æ¶ˆé™¤å¤šæ¬¡å†…å­˜è®¿é—®å¼€é”€")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
    
    return overall_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 